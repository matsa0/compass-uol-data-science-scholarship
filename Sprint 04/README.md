
<h1 align="center">
    <strong>SPRINT 04</strong>
</h1>

# üìù Exerc√≠cios

### ‚û£ Curso: Machine Learning com Amazon AWS e SageMaker

- **Se√ß√£o 03: Regress√£o Linear Learner e XGBoost**<br> A se√ß√£o 03 possui um exerc√≠cio que sintetiza todo o conte√∫do aprendido na se√ß√£o acerca principalmente do algoritmo `XGBoost. Por√©m, trabalhamos com **tratamento da base de dados**, **configura√ß√µes do SageMaker**, **treinamento** e **deploy** do XGBoost.

    [Clique para visualizar o c√≥digo](./exercicios/curso_aws_e_sagemaker/sec3/xgboost_credit_card.ipynb)


### ‚û£ Curso: Machine Learning e Data Science com Python de A a Z
No diret√≥rio de exerc√≠cios, coloquei alguns noteboooks que eu trabalhei durante o curso.

# üî¥ V√≠deo - [Desafio Sprint 04]

# üîé Evid√™ncias

### ‚û£ Curso: Machine Learning com Amazon AWS e SageMaker

- **Se√ß√£o 02: Introdu√ß√£o ao AWS**<br> Nesta se√ß√£o aprendi sobre os servi√ßos mais utilizados da AWS, como o SageMaker, S3 e IAM, entendendo um pouco das funcionalidades e prop√≥sitos de cada um deles.

    - **Amazon SageMaker:** Plataforma de machine learning que facilita a cria√ß√£o, treinamento e implanta√ß√£o de modelos. Integra-se com outros servi√ßos da AWS, como S3 e IAM, para oferecer escalabilidade e simplicidade no gerenciamento de modelos e dados.

    - **Amazon S3:** Servi√ßo de armazenamento escal√°vel e seguro. Aprendi sobre buckets, que s√£o como pastas para armazenamento de arquivos.
    
    - **AWS IAM:** Servi√ßo que gerencia usu√°rios, permiss√µes e fun√ß√µes para garantir a seguran√ßa dos recursos.

        Exemplo de cria√ß√£o de `bucket` via c√≥digo:

        ![Evidencia](./evidencias/curso_aws_sagemaker/sec2/criacao_bucket.png) 

- **Se√ß√£o 03: Regress√£o Linear Learner e XGBoost**<br> 

    - **Configura√ß√µes do SageMaker:** Realiza√ß√£o da configura√ß√£o pr√©via do SageMaker e buckets E3 para preparar os dados que ser√£o enviados atrav√©s do treinamento do modelo.

        Os dados est√£o sendo enviados de forma que os algoritmos integrados da AWS possam utiliz√°-los no treinamento de modelos.

        Um `buffer` √© criado para armazenar os dados em mem√≥ria e √© feito uma convers√£o para o `Dense Tensor` que √© o formato bin√°rio necess√°rio para os algoritmos integrados do SageMaker.

        ![Evidencia](./evidencias/curso_aws_sagemaker/sec3/preparacao_dados.png)

        Treinamento `Linear Learner` com a inst√¢nica **ml.m5.large**

        ![Evidencia](./evidencias/curso_aws_sagemaker/sec3/treinamento_linear_learner.png)

        2025-01-07 12:10:30 Completed - Training job completed<br>
        Training seconds: 150<br>
        Billable seconds: 150

        Cria√ß√£o de um modelo de `Deploy ` que pode ser utilizado para realiza√ß√£o de previs√µes.

        ![Evidencia](./evidencias/curso_aws_sagemaker/sec3/deploy.png)

        Avalia√ß√£o do modelo preditivo atrav√©s das m√©tricas Esse trecho de c√≥digo est√° avaliando a qualidade de um modelo preditivo utilizando as m√©tricas **MAE (Mean Absolute Error)** e **MSE (Mean Squared Error)**.

        Essas m√©tricas ajudam a avaliar o desempenho do modelo preditivo, indicando qu√£o pr√≥ximas est√£o as previs√µes (previsions) dos valores reais (y_test).

        - **MAE:** Indica o erro m√©dio absoluto, f√°cil de interpretar.
        - **MSE:** Penaliza mais os erros grandes, destacando previs√µes discrepantes.
    
        ![Evidencia](./evidencias/curso_aws_sagemaker/sec3/mae_mse.png)

    - **XGBoost:** O `Extreme Gradient Boosting` √© uma biblioteca de aprendizado de m√°quina baseada no algoritmo de `gradient boosting`, amplamente utilizada por sua efici√™ncia, flexibilidade e alto desempenho.

        ![Evidencia](./evidencias/curso_aws_sagemaker/sec3/treinamento_xgboost.png)

        2025-01-08 13:39:57 Uploading - Uploading generated training model<br>
        2025-01-08 13:39:57 Completed - Training job completed<br>
        Training seconds: 105<br>
        Billable seconds: 105

        √â interessante observar que o `RMSE(Root Mean Squared Error)` diminuiu consistentemente ao longo das itera√ß√µes, tanto para os dados de treinamento quanto de valida√ß√£o, o que mostra que **o modelo est√° aprendendo adequadamente**.

        - **Deploy e previs√µes utilizando o XGBoost**

        ![Evidencia](./evidencias/curso_aws_sagemaker/sec3/deploy_xgboost.png)

        - **Tuning:** processo de encontrar os melhores valores para os hiperpar√¢metros de um modelo de Machine Learning (ML), a fim de melhorar sua performance em um determinado conjunto de dados.

            Tendo como objetivo final a m√©trica `RMSE(Root Mean Squared Error)`, obtive os seguintes resultados:

            ![Evidencia](./evidencias/curso_aws_sagemaker/sec3/hyperparameters_rmse.png) 

            Best Training Job:

            ![Evidencia](./evidencias/curso_aws_sagemaker/sec3/btj.png)

            Hyperparameters:

            ![Evidencia](./evidencias/curso_aws_sagemaker/sec3/hyperparameters_xgboost_train.png)

- **Se√ß√£o 04: Classifica√ß√£o com Linear Learner e XGBoost**<br> O **XGBoost (Extreme Gradient Boosting)** √© uma biblioteca de aprendizado de m√°quina baseada no algoritmo de `gradient boosting`, amplamente utilizada por sua efici√™ncia, flexibilidade e alto desempenho.

    O XGBoost utiliza o gradient boosting, que combina v√°rios modelos fracos (geralmente `√°rvores de decis√£o`) de maneira sequencial e cada √°rvore √© constru√≠da para **corrigir os erros das √°rvores anteriores**, minimizando uma fun√ß√£o de perda.


- **Se√ß√£o 05: S√©ries temporais com DeepAR**<br>


- **Se√ß√£o 06: Outliers com Random Cut Forest**<br>


- **Se√ß√£o 07: PCA e agrupamento K-means**<br>


- **Se√ß√£o 08: Redes neurais artificiais - classifica√ß√£o de imagens**<br>


- **Se√ß√£o 09: Sagemaker com TensorFlow**<br>


- **Se√ß√£o 10: Endpoint externo**<br>


- **Se√ß√£o 12: Anexo 1: Redes neurais artificiais**<br>


- **Se√ß√£o 13: Anexo 2: Redes neurais convolucionais**<br>


- **Se√ß√£o 14: Anexo 3: Redes neurais recorrentes**<br>


### ‚û£ Curso: Machine Learning e Data Science com Python de A a Z

- **Se√ß√£o 14: Regress√£o Linear**<br> A regress√£o linear √© um conceito estat√≠stico que busca realizar a modelagem da rela√ß√£o entre vari√°veis num√©ricas, sendo separadas em uma `vari√°vel dependente(alvo)` e `vari√°veis independentes(preditores)`. O principal objetivo √© encontrar a **melhor linha reta** que mostre a rela√ß√£o entre essas vari√°veis e dizer o qu√£o bem a vari√°vel dependente pode ser **prevista** pelas vari√°veis preditoras.

    - **Regress√£o Linear Simples:** esse tipo de regress√£o linear relaciona uma vari√°vel dependendete a uma √∫nica vari√°vel independente.

        ![Evidencia](./evidencias/sec14/regressao_linear_simples.png)

    Prever o custo de um plano de sa√∫de baseado na idade de uma pessoa. O modelo foi criado tendo como vari√°vel dependente(y) o `custo` e independente(X) `idade`.

    A linha vermelha representa as previs√µes realizadas pelo modelo.

    - **Regress√£o Linear M√∫ltipla:** esse tipo de regress√£o linear relaciona uma vari√°vel dependendete a m√∫ltiplas vari√°veis independentes.

        Selecionando as vari√°veis independentes e a dependente(`price`). A ideia √© prever o pre√ßo de uma casa com base em diversas outras vari√°veis.

        ![Evidencia](./evidencias/sec14/regressao_multipla_variaveis.png)

        Previs√µes e `Mean Absolute Error`

        ![Evidencia](./evidencias/sec14/regressao_multipla.png)

- **Se√ß√£o 15: Outros tipos de regress√£o**<br>

    - **Regress√£o Polinomial:**<br> Uma extens√£o da regress√£o linear que permite capturar rela√ß√µes **n√£o lineares** entre as vari√°veis. √ötil quando a rela√ß√£o entre as vari√°veis n√£o √© linear e pode ser aproximada por uma **curva**.

        O par√¢metro `degree = 4`permite que o modelo se ajuste bem, mas em outros cen√°rios pode gerar overfitting em dados mais ruidosos.

        ![Evidencia](./evidencias/sec15/regressao_polinomial.png)

        ![Evidencia](./evidencias/sec15/regressao_polinomial2.png)

    - **Regress√£o com Random Forest**<br> O `Random Forest` faz parte do que se chama `Aprendizado em Conjunto(Ensemble Learning)`, que basicamente significa consultar diversas fontes para se obter um resultado mais preciso e robusto. O Random Forest utiliza de v√°rias `√°rvores de decis√£o` para construir um algoritmo mais "forte". Al√©m disso, utiliza a m√©dia(regress√£o) ou votos da maioria(classifica√ß√£o) para se obter uma resposta final.

        ![Evidencia](./evidencias/sec15/random_forest.png)

        A linha vermelha, que s√£o os `degraus` passam pr√≥ximas aos pontos reais, indicando que o modelo conseguiu capturar bem as varia√ß√µes presentes nos dados.

    - **Regress√£o com Redes Neurais Artificiais**<br> As RNAs s√£o amplamente utilizadas para resolver problemas complexos, nesse caso, a regress√£o. Os dados de entrada (X) s√£o escalados para melhorar o desempenho do treinamento, eles percorrem cada camada da rede realizando os c√°lculos de erros e ajuste de pesos.

        ![Evidencia](./evidencias/sec15/regressao_rna.png)

        O modelo parece ter se ajustado bem aos dados reais, dado que a linha vermelha segue o padr√£o dos pontos azuis.

- **Se√ß√£o 17: Algoritmo Apriori**<br> Antes de explicar o algoritmo Apriori, √© importante falar sobre as **regras de associa√ß√£o**.

    - **Regras de associa√ß√£o:**<br> As regras de associa√ß√£o tem como objetivo identificar **padr√µes e associa√ß√µes** entre conjunto de dados. Com esse conjunto de regras, √© poss√≠vel encontrar rela√ß√µes ocultas entre vari√°veis que geralmente n√£o s√£o esperadas.

    - √â poss√≠vel obter muitos **insights** que podem ser determin√≠sticos para **tomadas de decis√µe**s. Por exemplo, responder as seguintes perguntas:

        - Em que prateleira o biscoito de chocolate deve ser colocado para maximizar suas vendas?

        - Suco de uva costuma ser comprado com refrigerante?

        - Qual produto deve ser colocado em promo√ß√£o para uma venda casada com tomates?

    - **Apriori:**<br> O `Apriori` busca encontrar conjuntos frequentes de itens em transa√ß√µes e derivar regras de associa√ß√£o que indicam rela√ß√µes entre os itens.

    - 3 conceitos s√£o fundamentais de saber ao se estudar regras de associa√ß√£o e Apriori:

        `Suporte (Support):` Frequ√™ncia com que um conjunto de itens aparece nas transa√ß√µes.

        `Confian√ßa (Confidence):` Probabilidade de que, se um item AA √© comprado, BB tamb√©m seja.

        `Lift:` Mede a for√ßa da regra. Um valor maior que 1 indica uma associa√ß√£o positiva entre AA e BB.

        No Apriori, apenas os conjuntos de itens que **atendem ao suporte m√≠nimo definido** pelo usu√°rio s√£o mantidos. Al√©m disso, a partir desse conjunto de itens mantidos, o objetivo √© descobrir regras de associa√ß√£o com fator de **confian√ßa maior ou igual** ao estabelecido pelo usu√°rio.
    
    - **Utilizando o Apriori:**<br>
        Transformando o dataframe para uma lista e definindo as regras do **Apriori(Support, Confidence e Lift)**.

        ![Evidencia](./evidencias/sec17/apriori1.png)

        **Extra√ß√£o das regras** do algoritmo e transformando em um **dataframe**.

        ![Evidencia](./evidencias/sec17/apriori2.png)

        ![Evidencia](./evidencias/sec17/apriori3.png)

- **Se√ß√£o 20: Agrupamento com K-Means**<br> Agrupamento ou `clusteriza√ß√£o` √© uma t√©cnica muito utilizada em machine learning para identificar grupos (ou `clusters`) de dados que possuem **caracter√≠sticas semelhantes**. 

    O `K-Means` √© um dos algoritmos mais conhecidos para se trabalhar com clusteriza√ß√£o. Ele divide os dados em **K clusters**(definido pelo usu√°rio), calculando o `centroide`(centros de cada cluster) e ajustando at√© convergir.

    Ap√≥s encontrar os clusters, a m√©dia do cluster √© calculada e os centr√≥ides s√£o reposicionados.

    - **Utilizando o K-means:**<br> Utilizando uma pequena base que representa idades e sal√°rios, apliquei o algoritmo `K-means` para entender como seriam dividido **3 clusters**. A imagem mostra que foram criados 3 clusters com a m√©dia das idades sendo o centr√≥ide e a m√©dia dos sal√°rios recebidos.

        ![Evidencia](./evidencias/sec20/kmeans1.png)

        Os pontos maiores azuis s√£o os centr√≥ides dos clusters.

        ![Evidencia](./evidencias/sec20/kmeans2.png)

        Utilizando bases de dados rand√¥micas foi poss√≠vel plotar um gr√°fico que permitiu visualizar muito claramente a **separa√ß√£o de dados em cada cluster e os seus centr√≥ides**.

       ![Evidencia](./evidencias/sec20/kmeans3.png)

- **Se√ß√£o 21: Outros algoritmos de agrupamento**<br> Nesta se√ß√£o aprendemos sobre o agrupamento hier√°rquico, que basicamente tem como objetivo estabelecer uma **hierarquia de de agrupamentos**, onde √© criada uma estrutura em forma de √°rvore que indica o n√∫mero de clusters

    O `dendrograma`(√°rvore de clusters) exibe os grupos formados pelo agrupamento hier√°rquico em cada passo e em seus n√≠veis de similaridade.

    ![Evidencia](./evidencias/sec21/dendrograma_exemplo.png)

    Scatter Plot que evidencia os clusters aglomerativos criados em cima de uma base de cart√£o de cr√©dito, onde o Eixo X representa o limite disponibilizado no cart√£o de cr√©dito, e o Eixo Y o total gasto pelo cliente(Os dados est√£o normalizados).

    ![Evidencia](./evidencias/sec21/scatter_plot.png)

    - **Clientes conservadores (amarelo):** Baixo gasto apesar do alto limite dispon√≠vel.
    - **Clientes equilibrados (rosa):** Usam uma parte significativa de seus limites de forma consistente.
    - **Clientes dependentes (azul):** Altos gastos em rela√ß√£o a limites baixos.

    - **DBSCAN:**<br> DBSCAN √© um algoritmo de agrupamento baseado em densidade, agrupando pontos similares no mesmo espa√ßo, **n√£o sendo necess√°rio descobrir e especificar o n√∫mero de clusters**.

    Ele √© mais r√°pido e geralmente apresenta melhores resultados do que o K-Means, encontrando padr√µes n√£o lineares e sendo mais robusto contra outliers.

    ![Evidencia](./evidencias/sec21/dbscan.png)

    ![Evidencia](./evidencias/sec21/scatter_plot_dbscan.png)


- **Se√ß√£o 27: Sele√ß√£o de atributos**<br> A sele√ß√£o de atributos √© muito importante para a clusteriza√ß√£o e machine learnnig no geral, pois o seu objetivo √© **identificar os atributos mais relevantes** que influenciam os padr√µes ou comportamentos dos dados, reduzindo a dimensionalidade e melhorando a interpretabilidade.

    Considerando uma an√°lise onde queremos prever o sal√°rio de uma pessoa, vamos selecionar somente os atributos com o `threshold m√≠nimo de 0.05` e utilizar o algortimo de `Low Variance` em cima disso.

    ![Evidencia](./evidencias/sec27/low_variance.png)

    Cria√ß√£o de um dataframe com somente os √≠ndices selecionados, eles v√£o auxiliar nossa an√°lise.

    ![Evidencia](./evidencias/sec27/df_low_variance.png)

    Depois de realizar alguns processos, obtivemos esse modelo com **atributos selecionados** e **dimensionalidade reduzida**.

    ![Evidencia](./evidencias/sec27/low_variance2.png)

- **Se√ß√£o 28: Redu√ß√£o de dimensionalidade**<br> A **redu√ß√£o de dimensionalidade** √© uma t√©cnica de an√°lise de dados e estat√≠stica para reduzir o n√∫mero de vari√°veis aleat√≥rias que ser√£o inseridas em um modelo para treino, mantendo as suas **caracter√≠sticas essenciais**.

    A redu√ß√£o de dimensionalidade pode ser √∫til para: 

    - Melhorar o desempenho, a precis√£o e a interpretabilidade dos modelos
    - Economizar tempo e espa√ßo
    - Eliminar redund√¢ncias
    - Melhorar a efici√™ncia dos algoritmos de aprendizado de m√°quina

    Um dos algortimos mais utilizados √© o `PCA(Principal Component Analysis)`. Ele transforma os atributos originais em componentes principais, que s√£o combina√ß√µes lineares das vari√°veis.

    Na imagem abaixo √© poss√≠vel observar que o algoritmo fez uma combina√ß√£o de atributos, e o que eram 14, viraram 6 atributos, reduzindo a dimensionalidade.

    ![Evidencia](./evidencias/sec28/pca1.png)

    O modelo teve uma acur√°cia muito boa.

    ![Evidencia](./evidencias/sec28/pca2.png)

- **Se√ß√£o 29: Detec√ß√£o de Outliers**<br> Um `Outlier` √© um valor anormal, ou seja, fora do padr√£o(afastados da m√©dia). Esse valor anormal pode ser decorrente do acaso, **erro no preenchimento dos dados ou fraudes**, sendo necess√°rio trat√°-los seja **removendo o registro**, o que pode influenciar negativamente na base de dados, **substitu√≠-lo por outro valor**, ou **n√£o fazer nada**.

    Na imagem abaixo √© poss√≠vel visualizar os outliers(pontos) que representam valores anormais relacionados a idades em um dataframe, provavelmente decorrente de erros de digita√ß√£o ou de c√°lculo.

    ![Evidencia](./evidencias/sec29/boxplot.png)

    Scatter plot que mostra a rela√ß√£o entre a idade e a pontua√ß√£o de uma pessoa.

    ![Evidencia](./evidencias/sec29/scatter.png)

# üë®üèº‚Äçüéì Certificados

- Certificado do Curso **Machine Learning com Amazon AWS e SageMaker**
