
<h1 align="center">
    <strong>SPRINT 03</strong>
</h1>

# üìù Exerc√≠cios

No diret√≥rio de exerc√≠cios, coloquei alguns noteboooks que eu trabalhei durante o curso.

# üî¥ V√≠deo - [Desafio Sprint 04]

# üîé Evid√™ncias

### ‚û£ Curso: Machine Learning com Amazon AWS e SageMaker

### ‚û£ Curso: Machine Learning e Data Science com Python de A a Z

- **Se√ß√£o 14: Regress√£o Linear**<br> A regress√£o linear √© um conceito estat√≠stico que busca realizar a modelagem da rela√ß√£o entre vari√°veis num√©ricas, sendo separadas em uma `vari√°vel dependente(alvo)` e `vari√°veis independentes(preditores)`. O principal objetivo √© encontrar a **melhor linha reta** que mostre a rela√ß√£o entre essas vari√°veis e dizer o qu√£o bem a vari√°vel dependente pode ser **prevista** pelas vari√°veis preditoras.

    - **Regress√£o Linear Simples:** esse tipo de regress√£o linear relaciona uma vari√°vel dependendete a uma √∫nica vari√°vel independente.

        ![Evidencia](./evidencias/sec14/regressao_linear_simples.png)

    Prever o custo de um plano de sa√∫de baseado na idade de uma pessoa. O modelo foi criado tendo como vari√°vel dependente(y) o `custo` e independente(X) `idade`.

    A linha vermelha representa as previs√µes realizadas pelo modelo.

    - **Regress√£o Linear M√∫ltipla:** esse tipo de regress√£o linear relaciona uma vari√°vel dependendete a m√∫ltiplas vari√°veis independentes.

        Selecionando as vari√°veis independentes e a dependente(`price`). A ideia √© prever o pre√ßo de uma casa com base em diversas outras vari√°veis.

        ![Evidencia](./evidencias/sec14/regressao_multipla_variaveis.png)

        Previs√µes e `Mean Absolute Error`

        ![Evidencia](./evidencias/sec14/regressao_multipla.png)

- **Se√ß√£o 15: Outros tipos de regress√£o**<br>

    - **Regress√£o Polinomial:**<br> Uma extens√£o da regress√£o linear que permite capturar rela√ß√µes **n√£o lineares** entre as vari√°veis. √ötil quando a rela√ß√£o entre as vari√°veis n√£o √© linear e pode ser aproximada por uma **curva**.

        O par√¢metro `degree = 4`permite que o modelo se ajuste bem, mas em outros cen√°rios pode gerar overfitting em dados mais ruidosos.

        ![Evidencia](./evidencias/sec15/regressao_polinomial.png)

        ![Evidencia](./evidencias/sec15/regressao_polinomial2.png)

    - **Regress√£o com Random Forest**<br> O `Random Forest` faz parte do que se chama `Aprendizado em Conjunto(Ensemble Learning)`, que basicamente significa consultar diversas fontes para se obter um resultado mais preciso e robusto. O Random Forest utiliza de v√°rias `√°rvores de decis√£o` para construir um algoritmo mais "forte". Al√©m disso, utiliza a m√©dia(regress√£o) ou votos da maioria(classifica√ß√£o) para se obter uma resposta final.

        ![Evidencia](./evidencias/sec15/random_forest.png)

        A linha vermelha, que s√£o os `degraus` passam pr√≥ximas aos pontos reais, indicando que o modelo conseguiu capturar bem as varia√ß√µes presentes nos dados.

    - **Regress√£o com Redes Neurais Artificiais**<br> As RNAs s√£o amplamente utilizadas para resolver problemas complexos, nesse caso, a regress√£o. Os dados de entrada (X) s√£o escalados para melhorar o desempenho do treinamento, eles percorrem cada camada da rede realizando os c√°lculos de erros e ajuste de pesos.

        ![Evidencia](./evidencias/sec15/regressao_rna.png)

        O modelo parece ter se ajustado bem aos dados reais, dado que a linha vermelha segue o padr√£o dos pontos azuis.

- **Se√ß√£o 17: Algoritmo Apriori**<br> Antes de explicar o algoritmo Apriori, √© importante falar sobre as **regras de associa√ß√£o**.

    - **Regras de associa√ß√£o:**<br> As regras de associa√ß√£o tem como objetivo identificar **padr√µes e associa√ß√µes** entre conjunto de dados. Com esse conjunto de regras, √© poss√≠vel encontrar rela√ß√µes ocultas entre vari√°veis que geralmente n√£o s√£o esperadas.

    - √â poss√≠vel obter muitos **insights** que podem ser determin√≠sticos para **tomadas de decis√µe**s. Por exemplo, responder as seguintes perguntas:

        - Em que prateleira o biscoito de chocolate deve ser colocado para maximizar suas vendas?

        - Suco de uva costuma ser comprado com refrigerante?

        - Qual produto deve ser colocado em promo√ß√£o para uma venda casada com tomates?

    - **Apriori:**<br> O `Apriori` busca encontrar conjuntos frequentes de itens em transa√ß√µes e derivar regras de associa√ß√£o que indicam rela√ß√µes entre os itens.

    - 3 conceitos s√£o fundamentais de saber ao se estudar regras de associa√ß√£o e Apriori:

        `Suporte (Support):` Frequ√™ncia com que um conjunto de itens aparece nas transa√ß√µes.

        `Confian√ßa (Confidence):` Probabilidade de que, se um item AA √© comprado, BB tamb√©m seja.

        `Lift:` Mede a for√ßa da regra. Um valor maior que 1 indica uma associa√ß√£o positiva entre AA e BB.

        No Apriori, apenas os conjuntos de itens que **atendem ao suporte m√≠nimo definido** pelo usu√°rio s√£o mantidos. Al√©m disso, a partir desse conjunto de itens mantidos, o objetivo √© descobrir regras de associa√ß√£o com fator de **confian√ßa maior ou igual** ao estabelecido pelo usu√°rio.
    
    - **Utilizando o Apriori:**<br>
        Transformando o dataframe para uma lista e definindo as regras do **Apriori(Support, Confidence e Lift)**.

        ![Evidencia](./evidencias/sec17/apriori1.png)

        **Extra√ß√£o das regras** do algoritmo e transformando em um **dataframe**.

        ![Evidencia](./evidencias/sec17/apriori2.png)

        ![Evidencia](./evidencias/sec17/apriori3.png)

- **Se√ß√£o 20: Agrupamento com K-Means**<br> Agrupamento ou `clusteriza√ß√£o` √© uma t√©cnica muito utilizada em machine learning para identificar grupos (ou `clusters`) de dados que possuem **caracter√≠sticas semelhantes**. 

    O `K-Means` √© um dos algoritmos mais conhecidos para se trabalhar com clusteriza√ß√£o. Ele divide os dados em **K clusters**(definido pelo usu√°rio), calculando o `centroide`(centros de cada cluster) e ajustando at√© convergir.

    Ap√≥s encontrar os clusters, a m√©dia do cluster √© calculada e os centr√≥ides s√£o reposicionados.

    - **Utilizando o K-means:**<br> Utilizando uma pequena base que representa idades e sal√°rios, apliquei o algoritmo `K-means` para entender como seriam dividido **3 clusters**. A imagem mostra que foram criados 3 clusters com a m√©dia das idades sendo o centr√≥ide e a m√©dia dos sal√°rios recebidos.

        ![Evidencia](./evidencias/sec20/kmeans1.png)

        Os pontos maiores azuis s√£o os centr√≥ides dos clusters.

        ![Evidencia](./evidencias/sec20/kmeans2.png)

        Utilizando bases de dados rand√¥micas foi poss√≠vel plotar um gr√°fico que permitiu visualizar muito claramente a **separa√ß√£o de dados em cada cluster e os seus centr√≥ides**.

       ![Evidencia](./evidencias/sec20/kmeans3.png)

- **Se√ß√£o 21: Outros algoritmos de agrupamento**<br> Nesta se√ß√£o aprendemos sobre o agrupamento hier√°rquico, que basicamente tem como objetivo estabelecer uma **hierarquia de de agrupamentos**, onde √© criada uma estrutura em forma de √°rvore que indica o n√∫mero de clusters

    O `dendrograma`(√°rvore de clusters) exibe os grupos formados pelo agrupamento hier√°rquico em cada passo e em seus n√≠veis de similaridade.

    ![Evidencia](./evidencias/sec21/dendrograma_exemplo.png)

    Scatter Plot que evidencia os clusters aglomerativos criados em cima de uma base de cart√£o de cr√©dito, onde o Eixo X representa o limite disponibilizado no cart√£o de cr√©dito, e o Eixo Y o total gasto pelo cliente(Os dados est√£o normalizados).

    ![Evidencia](./evidencias/sec21/scatter_plot.png)

    - **Clientes conservadores (amarelo):** Baixo gasto apesar do alto limite dispon√≠vel.
    - **Clientes equilibrados (rosa):** Usam uma parte significativa de seus limites de forma consistente.
    - **Clientes dependentes (azul):** Altos gastos em rela√ß√£o a limites baixos.

    - **DBSCAN:**<br> DBSCAN √© um algoritmo de agrupamento baseado em densidade, agrupando pontos similares no mesmo espa√ßo, **n√£o sendo necess√°rio descobrir e especificar o n√∫mero de clusters**.

    Ele √© mais r√°pido e geralmente apresenta melhores resultados do que o K-Means, encontrando padr√µes n√£o lineares e sendo mais robusto contra outliers.

    ![Evidencia](./evidencias/sec21/dbscan.png)

    ![Evidencia](./evidencias/sec21/scatter_plot_dbscan.png)


- **Se√ß√£o 27: Sele√ß√£o de atributos**<br> A sele√ß√£o de atributos √© muito importante para a clusteriza√ß√£o e machine learnnig no geral, pois o seu objetivo √© **identificar os atributos mais relevantes** que influenciam os padr√µes ou comportamentos dos dados, reduzindo a dimensionalidade e melhorando a interpretabilidade.

    Considerando uma an√°lise onde queremos prever o sal√°rio de uma pessoa, vamos selecionar somente os atributos com o `threshold m√≠nimo de 0.05` e utilizar o algortimo de `Low Variance` em cima disso.

    ![Evidencia](./evidencias/sec27/low_variance.png)

    Cria√ß√£o de um dataframe com somente os √≠ndices selecionados, eles v√£o auxiliar nossa an√°lise.

    ![Evidencia](./evidencias/sec27/df_low_variance.png)

    Depois de realizar alguns processos, obtivemos esse modelo com **dimensionalidade reduzida**.

    ![Evidencia](./evidencias/sec27/low_variance2.png)

- **Se√ß√£o 28: Redu√ß√£o de dimensionalidade**<br>

- **Se√ß√£o 29: Detec√ß√£o de Outliers**<br>


# üë®üèº‚Äçüéì Certificados

- Certificado do Curso **Machine Learning com Amazon AWS e SageMaker**

- Certificado do Curso **Machine Learning e Data Science com Python de A a Z**

