
<h1 align="center">
    <strong>SPRINT 04</strong>
</h1>

# üî¥ V√≠deo - [Desafio Sprint 04]

# üìù Exerc√≠cios

### üß† Curso: Machine Learning com Amazon AWS e SageMaker

- **Se√ß√£o 03: Regress√£o Linear Learner e XGBoost**<br> A se√ß√£o 03 possui um exerc√≠cio que sintetiza todo o conte√∫do aprendido na se√ß√£o acerca principalmente do algoritmo `XGBoost. Por√©m, trabalhamos com **tratamento da base de dados**, **configura√ß√µes do SageMaker**, **treinamento** e **deploy** do XGBoost.

    [Clique para visualizar o c√≥digo](./exercicios/curso_aws_e_sagemaker/sec3/xgboost_credit_card.ipynb)

- **Se√ß√£o 04: Classifica√ß√£o com Linear Learner e XGBoost**<br> No exerc√≠cio dessa se√ß√£o trabalhamos com o treinamento de um modelo XGBoost, o seu deploy e an√°lise das previs√µes, a realiza√ß√£o do Tuning para uma melhor escolha dos hiperpar√¢metros e a constru√ß√£o de um novo modelo baseado no Tuning.

    [Clique para visualizar o c√≥digo](./exercicios/curso_aws_e_sagemaker/sec4/xgboost-census.ipynb)

- **Se√ß√£o 05: S√©ries temporais com DeepAR**<br> Neste exerc√≠cio utilizamos uma base de dados das a√ß√µes da Petrobras. A ideia era realizar uma previs√£o do pre√ßo das a√ß√µes atrav√©s de atributos que representam o pre√ßo das a√ß√µes di√°riamente, sendo interessante utilizar o DeepAR.

    [Clique para visualizar o c√≥digo](./exercicios/curso_aws_e_sagemaker/sec5/petr4-deepar.ipynb)

- **Se√ß√£o 06: Outliers com Random Cut Forest**<br> A ideia do exerc√≠cio da se√ß√£o 06 √© detectar os outliers de uma base de dados de a√ß√µes da Petrobras. Esse exerc√≠cio foi interessante para refor√ßar como utilizar o algoritmo Random Cut Forest.

    [Clique para visualizar o c√≥digo](./exercicios/curso_aws_e_sagemaker/sec6/petr4_outliers_random_cut.ipynb)

- **Se√ß√£o 07: PCA e agrupamento K-means**<br> O exerc√≠cio dessa se√ß√£o foi interessante para refor√ßar a utiliza√ß√£o do algoritmo PCA para reduzir a dimensionalidade do conjunto de dados e o Linear Learner para a classifica√ß√£o.


    [Clique para visualizar o c√≥digo](./exercicios/curso_aws_e_sagemaker/sec7/pca-census-classificacao.ipynb)

### üß† Curso: Machine Learning e Data Science com Python de A a Z
No diret√≥rio de exerc√≠cios, coloquei alguns noteboooks que eu trabalhei durante o curso.

# üîé Evid√™ncias

### üß† Curso: Machine Learning com Amazon AWS e SageMaker

- **Se√ß√£o 02: Introdu√ß√£o ao AWS**<br> Nesta se√ß√£o aprendi sobre os servi√ßos mais utilizados da AWS, como o SageMaker, S3 e IAM, entendendo um pouco das funcionalidades e prop√≥sitos de cada um deles.

    - **Amazon SageMaker:** Plataforma de machine learning que facilita a cria√ß√£o, treinamento e implanta√ß√£o de modelos. Integra-se com outros servi√ßos da AWS, como S3 e IAM, para oferecer escalabilidade e simplicidade no gerenciamento de modelos e dados.

    - **Amazon S3:** Servi√ßo de armazenamento escal√°vel e seguro. Aprendi sobre buckets, que s√£o como pastas para armazenamento de arquivos.
    
    - **AWS IAM:** Servi√ßo que gerencia usu√°rios, permiss√µes e fun√ß√µes para garantir a seguran√ßa dos recursos.

        Exemplo de cria√ß√£o de `bucket` via c√≥digo:

        ![Evidencia](./evidencias/curso_aws_sagemaker/sec2/criacao_bucket.png) 

- **Se√ß√£o 03: Regress√£o Linear Learner e XGBoost**<br> 

    - **Configura√ß√µes do SageMaker:** Realiza√ß√£o da configura√ß√£o pr√©via do SageMaker e buckets E3 para preparar os dados que ser√£o enviados atrav√©s do treinamento do modelo.

        Os dados est√£o sendo enviados de forma que os algoritmos integrados da AWS possam utiliz√°-los no treinamento de modelos.

        Um `buffer` √© criado para armazenar os dados em mem√≥ria e √© feito uma convers√£o para o `Dense Tensor` que √© o formato bin√°rio necess√°rio para os algoritmos integrados do SageMaker.

        ![Evidencia](./evidencias/curso_aws_sagemaker/sec3/preparacao_dados.png)

        Treinamento `Linear Learner` com a inst√¢nica **ml.m5.large**

        ![Evidencia](./evidencias/curso_aws_sagemaker/sec3/treinamento_linear_learner.png)

        2025-01-07 12:10:30 Completed - Training job completed<br>
        Training seconds: 150<br>
        Billable seconds: 150

        Cria√ß√£o de um modelo de `Deploy ` que pode ser utilizado para realiza√ß√£o de previs√µes.

        ![Evidencia](./evidencias/curso_aws_sagemaker/sec3/deploy.png)

        Avalia√ß√£o do modelo preditivo atrav√©s das m√©tricas Esse trecho de c√≥digo est√° avaliando a qualidade de um modelo preditivo utilizando as m√©tricas **MAE (Mean Absolute Error)** e **MSE (Mean Squared Error)**.

        Essas m√©tricas ajudam a avaliar o desempenho do modelo preditivo, indicando qu√£o pr√≥ximas est√£o as previs√µes (previsions) dos valores reais (y_test).

        - **MAE:** Indica o erro m√©dio absoluto, f√°cil de interpretar.
        - **MSE:** Penaliza mais os erros grandes, destacando previs√µes discrepantes.
    
        ![Evidencia](./evidencias/curso_aws_sagemaker/sec3/mae_mse.png)

    - **XGBoost:** O `Extreme Gradient Boosting` √© uma biblioteca de aprendizado de m√°quina baseada no algoritmo de `gradient boosting`, amplamente utilizada por sua efici√™ncia, flexibilidade e alto desempenho.

        ![Evidencia](./evidencias/curso_aws_sagemaker/sec3/treinamento_xgboost.png)

        2025-01-08 13:39:57 Uploading - Uploading generated training model<br>
        2025-01-08 13:39:57 Completed - Training job completed<br>
        Training seconds: 105<br>
        Billable seconds: 105

        √â interessante observar que o `RMSE(Root Mean Squared Error)` diminuiu consistentemente ao longo das itera√ß√µes, tanto para os dados de treinamento quanto de valida√ß√£o, o que mostra que **o modelo est√° aprendendo adequadamente**.

        - **Deploy e previs√µes utilizando o XGBoost**

        ![Evidencia](./evidencias/curso_aws_sagemaker/sec3/deploy_xgboost.png)

        - **Tuning:** processo de encontrar os melhores valores para os hiperpar√¢metros de um modelo de Machine Learning (ML), a fim de melhorar sua performance em um determinado conjunto de dados.

            Tendo como objetivo final a m√©trica `RMSE(Root Mean Squared Error)`, obtive os seguintes resultados:

            ![Evidencia](./evidencias/curso_aws_sagemaker/sec3/hyperparameters_rmse.png) 

            Best Training Job:

            ![Evidencia](./evidencias/curso_aws_sagemaker/sec3/btj.png)

            Hyperparameters:

            ![Evidencia](./evidencias/curso_aws_sagemaker/sec3/hyperparameters_xgboost_train.png)

- **Se√ß√£o 04: Classifica√ß√£o com Linear Learner e XGBoost**<br> O **XGBoost (Extreme Gradient Boosting)** √© uma biblioteca de aprendizado de m√°quina baseada no algoritmo de `gradient boosting`, amplamente utilizada por sua efici√™ncia, flexibilidade e alto desempenho.

    O XGBoost utiliza o gradient boosting, que combina v√°rios modelos fracos (geralmente `√°rvores de decis√£o`) de maneira sequencial e cada √°rvore √© constru√≠da para **corrigir os erros das √°rvores anteriores**, minimizando uma fun√ß√£o de perda.

    Nessa se√ß√£o tamb√©m somos introduzidos ao conceito de `Regress√£o Log√≠stica`, onde seu objetivo √© **prever classes**, diferentemente da regress√£o que era prever n√∫meros. Ela √© muito utilizada para resolver problemas de classifica√ß√£o bin√°ria ou seja, quando o objetivo √© prever se algo pertence a uma de duas categorias **(ex.: "sim" ou "n√£o", "verdadeiro" ou "falso", "1" ou "0")**.

    Exemplo de cria√ß√£o e treinamento de um modelo `Linear Learner`

    ![Evidencia](./evidencias/curso_aws_sagemaker/sec4/train_linear_learner.png)

    Ap√≥s o deploy e realiza√ß√£o das previs√µes, obtive esses valores para as m√©tricas na imagem abaixo, o que eu acredito estar **incorreto**. Isso evidencia que o modelo n√£o foi bem treinado. Por√©m, para evitar maiores custos, n√£o criei outro endpoint, mas acredito que alterar os hiperpar√¢metros como o `feature_dim` e `num_models` poderia trazer resultados mais coerentes.

    ![Evidencia](./evidencias/curso_aws_sagemaker/sec4/results_linear_learner.png)

- **Se√ß√£o 05: S√©ries temporais com DeepAR**<br> S√©ries temporais em ci√™ncia de dados, se referem a dados que s√£o registrados com base em intervalos de tempo regulares. A principal caracter√≠stica deles √© a **depend√™ncia temporal**, pois valores futuros podem ser influenciados por valores passados.

    No curso estudamos s√©ries temporais com o algoritmo `DeepAR` em uma base de dados de vendas de bicicletas em um dado per√≠odo de tempo. A frequ√™ncia de tempo trabalhada foi **di√°ria**.

    ![Evidencia](./evidencias/curso_aws_sagemaker/sec5/df.png)

    O DeepAR √© um modelo desenvolvido pela Amazon que se destaca em previs√µes de s√©ries temporais, muito utilizado quando se busca prever valores futuros de maneira precisa.

    Exemplo de treinamento do modelo:<br>
    Interessante ressaltar a defini√ß√£o de **hiperpar√¢metros** de forma separada. A visualiza√ß√£o da documenta√ß√£o √© muito √∫til para descobrir par√¢metros **obrigat√≥rios** e **opcionais**.

    ![Evidencia](./evidencias/curso_aws_sagemaker/sec5/train_deepar.png)

    Exemplo de deploy(cria√ß√£o do **endpoint**):

    ![Evidencia](./evidencias/curso_aws_sagemaker/sec5/deploy.png)

    Resultado gr√°fico das previs√µes do DeepAR.<br> 
    
    ![Evidencia](./evidencias/curso_aws_sagemaker/sec5/grafico.png)
    
    A `mediana`(linha vermelha) √© a previs√£o central do modelo, ou seja, o valor mais prov√°vel estimado.

    O `alvo`(linha azul) s√£o os valores reais observados na s√©rie temporal, usados para comparar com as previs√µes.

    O `intervalo de confian√ßa`(√°rea amarela) evidencia  a incerteza das previs√µes, delimitada pelos percentis 10 e 90 definidos.

    √â poss√≠vel observar, pela aproxima√ß√£o da linha vermelha √† azul, que o modelo **prev√™ bem os valores futuros**.

- **Se√ß√£o 06: Outliers com Random Cut Forest**<br> Os Outliers como j√° sabemos s√£o valores fora do padr√£o, ou muito discrepantes dos demais da base de dados.
    
    O `Random Cut Forest` √© muito utilizado para a detec√ß√£o de outliers em dados de alta dimensionalidade ou s√©ries temporais(como visto na se√ß√£o anterior). O **RCF** baseia-se na constru√ß√£o de florestas de √°rvores que particionam os dados de forma aleat√≥ria sempre avaliando o qu√£o **fora do normal** cada ponto √© em **rela√ß√£o ao conjunto de dados** e fazendo sua detec√ß√£o. 
    
    Ele associa um `anomaly score` para cada um dos registros. Valores baixos indicam registros normal, valores altos indicam anomalia no registro. Por esse motivo, o RCF √© t√£o eficaz para a detec√ß√£o de outliers.

    ![Evidencia](./evidencias/curso_aws_sagemaker/sec6/treinamento_rcf.png)

    Detalhes do treinamento:<br>
    2025-01-10 14:19:13 Training - Training image download completed. Training in progress.<br>
    2025-01-10 14:19:13 Uploading - Uploading generated training model<br>
    2025-01-10 14:19:25 Completed - Training job completed<br>
    Training seconds: 140<br>
    Billable seconds: 53<br>
    Managed Spot Training savings: 62.1%<br>

    Interessante notar que o par√¢metro `use_spot_instances = True` salvou 62.1% de custos.

    Atrav√©s da documenta√ß√£o do RCF, foi poss√≠vel saber que os scores que possu√≠rem valores **acima de 3 desvios padr√µes com base na m√©dia** s√£o considerados valores anormais. Atrav√©s dessa informa√ß√£o, foi poss√≠vel fazer esse calculo e obter um dataframe com os outliers.

    ![Evidencia](./evidencias/curso_aws_sagemaker/sec6/calculo_outliers.png)

- **Se√ß√£o 07: PCA e agrupamento K-means**<br> `PCA` √© um algoritmo que tem como objetivo **reduzir a dimensionalidade** enquanto preserva a maior quantidade poss√≠vel de variabilidade (informa√ß√£o) presente no dataset. Quando h√° muitos atributos com alta correla√ß√£o em uma base de dados, √© um **sinal** que podemos reduzir a sua dimensionalidade, pois muitas colunas est√£o correlacionadas.

    ![Evidencia](./evidencias/curso_aws_sagemaker/sec7/pca_train.png)

    2025-01-11 11:43:53 Completed - Training job completed<br>
    Training seconds: 120<br>
    Billable seconds: 120<br>

    Deploy:

    ![Evidencia](./evidencias/curso_aws_sagemaker/sec7/deploy_reducao.png)

    Redu√ß√£o de dimensionalidade(de 25 para 2 colunas):

    ![Evidencia](./evidencias/curso_aws_sagemaker/sec7/deploy_reucao2.png)

    `K-Means` √© um dos algoritmos mais conhecidos para se trabalhar com clusteriza√ß√£o. Ele divide os dados em **K clusters**(definido pelo usu√°rio), calculando o centroide(centros de cada cluster) e ajustando at√© convergir.

    Ap√≥s encontrar os clusters, a m√©dia do cluster √© calculada e os **centr√≥ides s√£o reposicionados**.

    Treinamento do K-Means:

    ![Evidencia](./evidencias/curso_aws_sagemaker/sec7/kmeans_train.png)

    Scatter Plot para **k = 4**:

    ![Evidencia](./evidencias/curso_aws_sagemaker/sec7/clusters_Figure.png)

    - Os **pontos verdes** s√£o pessoas que gastam pouco no cart√£o de cr√©dito e possuem pouco limite;
    - Os **pontos amarelos** s√£o pessoas que possuem um limite maior, mas gastam pouco;
    - Os **pontos vermelhos** s√£o pessoas que ficam na parte central, ou seja, possuem um limite m√©dio de cart√£o de cr√©dito e tamb√©m gastam de forma moderada.
    - Os **pontos azuis** s√£o pessoas que possuem um limite alto e gastam muito no cart√£o de cr√©dito. 

- **Se√ß√£o 08: Redes neurais artificiais - classifica√ß√£o de imagens**<br> Na se√ß√£o 08 vemos como configurar, treinar e realizar deploy de uma rede neural convolucional(cnn) para trabalhar com classifica√ß√£o de imagens.

    ![Evidencia](./evidencias/curso_aws_sagemaker/sec8/treinamento_cnn.png)

    Foi realizado uma aplica√ß√£o do modelo treinado em imagens individuais para verificar predi√ß√µes, al√©m de identificar a categoria ou r√≥tulo correspondente √† maior probabilidade em uma previs√£o de classifica√ß√£o, baseando-se no resultado gerado pelo modelo.

    ![Evidencia](./evidencias/curso_aws_sagemaker/sec8/probab_maior.png)

- **Se√ß√£o 09: Sagemaker com TensorFlow**<br> Na se√ß√£o trablha-se com o treinamento e a avalia√ß√£o de uma rede neural usando a base de dados **MNIST** com o `TensorFlow` e o `Keras`.

    A base MNIST, que cont√©m imagens de d√≠gitos manuscritos **(28x28 pixels)**, √© carregada e dividida em dados de treinamento e teste.

    ![Evidencia](./evidencias/curso_aws_sagemaker/sec9/mnist.png)

    Em resumo, a se√ß√£o aborda a cria√ß√£o e treinamento de uma rede neural para classifica√ß√£o de d√≠gitos atrav√©s da base MNIST. Tamb√©m √© avaliado o desempenho do modelo nos dados de teste para medir sua acur√°cia e por fim, ele √© salvo para reutiliza√ß√£o e predi√ß√µes futuras.

    ![Evidencia](./evidencias/curso_aws_sagemaker/sec9/rede_neural.png)

    Resumo da arquitetura da rede neural criada:

    ![Evidencia](./evidencias/curso_aws_sagemaker/sec9/reded_neural_summary.png)

    - `dense_9`: Primeira camada oculta com 397 neur√¥nios e fun√ß√£o de ativa√ß√£o ReLU.
    - `dense_10`: Segunda camada oculta com 397 neur√¥nios e fun√ß√£o de ativa√ß√£o ReLU.
    - `dense_11`: Camada de sa√≠da com 10 neur√¥nios e fun√ß√£o de ativa√ß√£o softmax (para classifica√ß√£o multiclasse).

- **Se√ß√£o 10: Endpoint externo**<br> Na se√ß√£o 10 se trabalha com o conceito de **Endpoint externo**, que nesse contexto √© uma interface que conecta a aplica√ß√£o do cliente a um modelo de Machine Learning implantado na nuvem.

    Nessa se√ß√£o, utiliza-se um endpoint externo para realizar previs√µes com um modelo treinado e implantado no Amazon SageMaker.

    O modelo treinado usando o algoritmo `XGBoost` √© utilizado para fazer previs√µes.

    ![Evidencia](./evidencias/curso_aws_sagemaker/sec10/endpoint_externo.png)

- **Se√ß√£o 11: Autopilot - aprendizagem autom√°tica**<br> O `SageMaker Autopilot` automatiza o processo de treinamento de modelos de Machine Learning. Ele permite que voc√™ envie **dados brutos** e obtenha um **modelo treinado** com base nesses dados, sem a necessidade de conhecimento profundo sobre algoritmos ou engenharia de features.

    O que voc√™ precisa fazer √© somente configurar o experimento, como o bucket, o dataset, vari√°vel alvo, o tipo de problema machine learning, tempo m√°ximo de execu√ß√£o e entre outras configura√ß√µes que voc√™ geralmente realiza, por√©m sendo preciso 0 linhas de c√≥digo.   

    ![Evidencia](./evidencias/curso_aws_sagemaker/sec11/experiment.png)


- **Se√ß√£o 12: Anexo 1: Redes neurais artificiais**<br> As Redes neurais artificiais(RNAs) s√£o modelos computacionais que tentam replicar o funcionamento do c√©rebro humano. As RNAs possuem as unidades conectadas chamadas de **neur√¥nios**, a organiza√ß√£o em **camadas** que processam dados para resolver problemas complexos como classifica√ß√£o, regress√£o, detec√ß√£o de padr√µes e tomada de decis√£o.

    Existem v√°rios tipos de redes neurais como a simples, multicamadas e entre outras.

    A rede **Perceptron Simples** √© um modelo b√°sicox om uma √∫nica camada de sa√≠da.

    A rede **Perceptron Multicamadas(MLP)** consiste em v√°rias camadas (entrada, ocultas, sa√≠da).

    Durante o treinamento, a rede ajusta os pesos e os vieses para minimizar o erro. O conceito de **backpropagation** √© um c√°lculo do gradiente do erro em rela√ß√£o aos pesos, usando o algoritmo do gradiente descendente.

- **Se√ß√£o 13: Anexo 2: Redes neurais convolucionais**<br> As redes neurais convolucionais s√£o projetadas para lidar com dados com grande estrutura de grade. Um √≥timo exemplo, s√£o imagens, v√≠deos, voz entre outros.

    As `CNNs` possuem processos importantes de destacar como o `Pooling(amostragem)`, que tem como objetivo reduzir a **dimensionalidade da entrada**, mantendo as caracter√≠sticas mais importantes. Ele tem uma fun√ß√£o muito importante em diminuir o n√∫mero de par√¢metros e c√°lculos, ajudando a evitar **overfitting**.

    Outro processo, √© o `Flattening`, que √© necess√°rio para tratar da sa√≠da de uma camada convolucional ou de uma camada de pooling (geralmente em **formato matricial ou tensorial**) √© convertida em um **vetor unidimensional**.


- **Se√ß√£o 14: Anexo 3: Redes neurais recorrentes**<br> As redes neurais recorrentes s√£o projetadas para lidar com dados **sequenciais e temporais**, como s√©ries temporais, texto, √°udio ou v√≠deo. A principal caracteristica delas √© a capacidade de eter informa√ß√µes sobre entradas anteriores ao processar uma nova entrada, permitindo que a rede tenha uma **"mem√≥ria" de curto prazo**.

    Um fato importante sobre as redes neurais recorrentes, √© que durante o treinamento, os gradientes podem se tornar **extremamente pequenos** (desaparecer) ou **extremamente grandes** (explodir), dificultando o aprendizado de depend√™ncias temporais de longo prazo.

    AS `LSTMs(Long Short-Term Memory)` s√£o um tipo de RNN que usa c√©lulas de mem√≥ria e mecanismos de "portas" para aprender depend√™ncias de longo prazo e curto prazo. 

### üß† Curso: Machine Learning e Data Science com Python de A a Z

- **Se√ß√£o 14: Regress√£o Linear**<br> A regress√£o linear √© um conceito estat√≠stico que busca realizar a modelagem da rela√ß√£o entre vari√°veis num√©ricas, sendo separadas em uma `vari√°vel dependente(alvo)` e `vari√°veis independentes(preditores)`. O principal objetivo √© encontrar a **melhor linha reta** que mostre a rela√ß√£o entre essas vari√°veis e dizer o qu√£o bem a vari√°vel dependente pode ser **prevista** pelas vari√°veis preditoras.

    - **Regress√£o Linear Simples:** esse tipo de regress√£o linear relaciona uma vari√°vel dependendete a uma √∫nica vari√°vel independente.

        ![Evidencia](./evidencias/curso_machine_learning_e_data_science/sec14/regressao_linear_simples.png)

    Prever o custo de um plano de sa√∫de baseado na idade de uma pessoa. O modelo foi criado tendo como vari√°vel dependente(y) o `custo` e independente(X) `idade`.

    A linha vermelha representa as previs√µes realizadas pelo modelo.

    - **Regress√£o Linear M√∫ltipla:** esse tipo de regress√£o linear relaciona uma vari√°vel dependendete a m√∫ltiplas vari√°veis independentes.

        Selecionando as vari√°veis independentes e a dependente(`price`). A ideia √© prever o pre√ßo de uma casa com base em diversas outras vari√°veis.

        ![Evidencia](./evidencias/curso_machine_learning_e_data_science/sec14/regressao_multipla_variaveis.png)

        Previs√µes e `Mean Absolute Error`

        ![Evidencia](./evidencias/curso_machine_learning_e_data_science/sec14/regressao_multipla.png)

- **Se√ß√£o 15: Outros tipos de regress√£o**<br>

    - **Regress√£o Polinomial:**<br> Uma extens√£o da regress√£o linear que permite capturar rela√ß√µes **n√£o lineares** entre as vari√°veis. √ötil quando a rela√ß√£o entre as vari√°veis n√£o √© linear e pode ser aproximada por uma **curva**.

        O par√¢metro `degree = 4`permite que o modelo se ajuste bem, mas em outros cen√°rios pode gerar overfitting em dados mais ruidosos.

        ![Evidencia](./evidencias/curso_machine_learning_e_data_science/sec15/regressao_polinomial.png)

        ![Evidencia](./evidencias/curso_machine_learning_e_data_science/sec15/regressao_polinomial2.png)

    - **Regress√£o com Random Forest**<br> O `Random Forest` faz parte do que se chama `Aprendizado em Conjunto(Ensemble Learning)`, que basicamente significa consultar diversas fontes para se obter um resultado mais preciso e robusto. O Random Forest utiliza de v√°rias `√°rvores de decis√£o` para construir um algoritmo mais "forte". Al√©m disso, utiliza a m√©dia(regress√£o) ou votos da maioria(classifica√ß√£o) para se obter uma resposta final.

        ![Evidencia](./evidencias/curso_machine_learning_e_data_science/sec15/random_forest.png)

        A linha vermelha, que s√£o os `degraus` passam pr√≥ximas aos pontos reais, indicando que o modelo conseguiu capturar bem as varia√ß√µes presentes nos dados.

    - **Regress√£o com Redes Neurais Artificiais**<br> As RNAs s√£o amplamente utilizadas para resolver problemas complexos, nesse caso, a regress√£o. Os dados de entrada (X) s√£o escalados para melhorar o desempenho do treinamento, eles percorrem cada camada da rede realizando os c√°lculos de erros e ajuste de pesos.

        ![Evidencia](./evidencias/curso_machine_learning_e_data_science/sec15/regressao_rna.png)

        O modelo parece ter se ajustado bem aos dados reais, dado que a linha vermelha segue o padr√£o dos pontos azuis.

- **Se√ß√£o 17: Algoritmo Apriori**<br> Antes de explicar o algoritmo Apriori, √© importante falar sobre as **regras de associa√ß√£o**.

    - **Regras de associa√ß√£o:**<br> As regras de associa√ß√£o tem como objetivo identificar **padr√µes e associa√ß√µes** entre conjunto de dados. Com esse conjunto de regras, √© poss√≠vel encontrar rela√ß√µes ocultas entre vari√°veis que geralmente n√£o s√£o esperadas.

    - √â poss√≠vel obter muitos **insights** que podem ser determin√≠sticos para **tomadas de decis√µe**s. Por exemplo, responder as seguintes perguntas:

        - Em que prateleira o biscoito de chocolate deve ser colocado para maximizar suas vendas?

        - Suco de uva costuma ser comprado com refrigerante?

        - Qual produto deve ser colocado em promo√ß√£o para uma venda casada com tomates?

    - **Apriori:**<br> O `Apriori` busca encontrar conjuntos frequentes de itens em transa√ß√µes e derivar regras de associa√ß√£o que indicam rela√ß√µes entre os itens.

    - 3 conceitos s√£o fundamentais de saber ao se estudar regras de associa√ß√£o e Apriori:

        `Suporte (Support):` Frequ√™ncia com que um conjunto de itens aparece nas transa√ß√µes.

        `Confian√ßa (Confidence):` Probabilidade de que, se um item AA √© comprado, BB tamb√©m seja.

        `Lift:` Mede a for√ßa da regra. Um valor maior que 1 indica uma associa√ß√£o positiva entre AA e BB.

        No Apriori, apenas os conjuntos de itens que **atendem ao suporte m√≠nimo definido** pelo usu√°rio s√£o mantidos. Al√©m disso, a partir desse conjunto de itens mantidos, o objetivo √© descobrir regras de associa√ß√£o com fator de **confian√ßa maior ou igual** ao estabelecido pelo usu√°rio.
    
    - **Utilizando o Apriori:**<br>
        Transformando o dataframe para uma lista e definindo as regras do **Apriori(Support, Confidence e Lift)**.

        ![Evidencia](./evidencias/curso_machine_learning_e_data_science/sec17/apriori1.png)

        **Extra√ß√£o das regras** do algoritmo e transformando em um **dataframe**.

        ![Evidencia](./evidencias/curso_machine_learning_e_data_science/sec17/apriori2.png)

        ![Evidencia](./evidencias/curso_machine_learning_e_data_science/sec17/apriori3.png)

- **Se√ß√£o 20: Agrupamento com K-Means**<br> Agrupamento ou `clusteriza√ß√£o` √© uma t√©cnica muito utilizada em machine learning para identificar grupos (ou `clusters`) de dados que possuem **caracter√≠sticas semelhantes**. 

    O `K-Means` √© um dos algoritmos mais conhecidos para se trabalhar com clusteriza√ß√£o. Ele divide os dados em **K clusters**(definido pelo usu√°rio), calculando o `centroide`(centros de cada cluster) e ajustando at√© convergir.

    Ap√≥s encontrar os clusters, a m√©dia do cluster √© calculada e os centr√≥ides s√£o reposicionados.

    - **Utilizando o K-means:**<br> Utilizando uma pequena base que representa idades e sal√°rios, apliquei o algoritmo `K-means` para entender como seriam dividido **3 clusters**. A imagem mostra que foram criados 3 clusters com a m√©dia das idades sendo o centr√≥ide e a m√©dia dos sal√°rios recebidos.

        ![Evidencia](./evidencias/curso_machine_learning_e_data_science/sec20/kmeans1.png)

        Os pontos maiores azuis s√£o os centr√≥ides dos clusters.

        ![Evidencia](./evidencias/curso_machine_learning_e_data_science/sec20/kmeans2.png)

        Utilizando bases de dados rand√¥micas foi poss√≠vel plotar um gr√°fico que permitiu visualizar muito claramente a **separa√ß√£o de dados em cada cluster e os seus centr√≥ides**.

       ![Evidencia](./evidencias/ curso_machine_learning_e_data_science/sec20/kmeans3.png)

- **Se√ß√£o 21: Outros algoritmos de agrupamento**<br> Nesta se√ß√£o aprendemos sobre o agrupamento hier√°rquico, que basicamente tem como objetivo estabelecer uma **hierarquia de de agrupamentos**, onde √© criada uma estrutura em forma de √°rvore que indica o n√∫mero de clusters

    O `dendrograma`(√°rvore de clusters) exibe os grupos formados pelo agrupamento hier√°rquico em cada passo e em seus n√≠veis de similaridade.

    ![Evidencia](./evidencias/curso_machine_learning_e_data_science/sec21/dendrograma_exemplo.png)

    Scatter Plot que evidencia os clusters aglomerativos criados em cima de uma base de cart√£o de cr√©dito, onde o Eixo X representa o limite disponibilizado no cart√£o de cr√©dito, e o Eixo Y o total gasto pelo cliente(Os dados est√£o normalizados).

    ![Evidencia](./evidencias/curso_machine_learning_e_data_science/sec21/scatter_plot.png)

    - **Clientes conservadores (amarelo):** Baixo gasto apesar do alto limite dispon√≠vel.
    - **Clientes equilibrados (rosa):** Usam uma parte significativa de seus limites de forma consistente.
    - **Clientes dependentes (azul):** Altos gastos em rela√ß√£o a limites baixos.

    - **DBSCAN:**<br> DBSCAN √© um algoritmo de agrupamento baseado em densidade, agrupando pontos similares no mesmo espa√ßo, **n√£o sendo necess√°rio descobrir e especificar o n√∫mero de clusters**.

    Ele √© mais r√°pido e geralmente apresenta melhores resultados do que o K-Means, encontrando padr√µes n√£o lineares e sendo mais robusto contra outliers.

    ![Evidencia](./evidencias/curso_machine_learning_e_data_science/sec21/dbscan.png)

    ![Evidencia](./evidencias/curso_machine_learning_e_data_science/sec21/scatter_plot_dbscan.png)


- **Se√ß√£o 27: Sele√ß√£o de atributos**<br> A sele√ß√£o de atributos √© muito importante para a clusteriza√ß√£o e machine learnnig no geral, pois o seu objetivo √© **identificar os atributos mais relevantes** que influenciam os padr√µes ou comportamentos dos dados, reduzindo a dimensionalidade e melhorando a interpretabilidade.

    Considerando uma an√°lise onde queremos prever o sal√°rio de uma pessoa, vamos selecionar somente os atributos com o `threshold m√≠nimo de 0.05` e utilizar o algortimo de `Low Variance` em cima disso.

    ![Evidencia](./evidencias/curso_machine_learning_e_data_science/sec27/low_variance.png)

    Cria√ß√£o de um dataframe com somente os √≠ndices selecionados, eles v√£o auxiliar nossa an√°lise.

    ![Evidencia](./evidencias/curso_machine_learning_e_data_science/sec27/df_low_variance.png)

    Depois de realizar alguns processos, obtivemos esse modelo com **atributos selecionados** e **dimensionalidade reduzida**.

    ![Evidencia](./evidencias/curso_machine_learning_e_data_science/sec27/low_variance2.png)

- **Se√ß√£o 28: Redu√ß√£o de dimensionalidade**<br> A **redu√ß√£o de dimensionalidade** √© uma t√©cnica de an√°lise de dados e estat√≠stica para reduzir o n√∫mero de vari√°veis aleat√≥rias que ser√£o inseridas em um modelo para treino, mantendo as suas **caracter√≠sticas essenciais**.

    A redu√ß√£o de dimensionalidade pode ser √∫til para: 

    - Melhorar o desempenho, a precis√£o e a interpretabilidade dos modelos
    - Economizar tempo e espa√ßo
    - Eliminar redund√¢ncias
    - Melhorar a efici√™ncia dos algoritmos de aprendizado de m√°quina

    Um dos algortimos mais utilizados √© o `PCA(Principal Component Analysis)`. Ele transforma os atributos originais em componentes principais, que s√£o combina√ß√µes lineares das vari√°veis.

    Na imagem abaixo √© poss√≠vel observar que o algoritmo fez uma combina√ß√£o de atributos, e o que eram 14, viraram 6 atributos, reduzindo a dimensionalidade.

    ![Evidencia](./evidencias/curso_machine_learning_e_data_science/sec28/pca1.png)

    O modelo teve uma acur√°cia muito boa.

    ![Evidencia](./evidencias/curso_machine_learning_e_data_science/sec28/pca2.png)

- **Se√ß√£o 29: Detec√ß√£o de Outliers**<br> Um `Outlier` √© um valor anormal, ou seja, fora do padr√£o(afastados da m√©dia). Esse valor anormal pode ser decorrente do acaso, **erro no preenchimento dos dados ou fraudes**, sendo necess√°rio trat√°-los seja **removendo o registro**, o que pode influenciar negativamente na base de dados, **substitu√≠-lo por outro valor**, ou **n√£o fazer nada**.

    Na imagem abaixo √© poss√≠vel visualizar os outliers(pontos) que representam valores anormais relacionados a idades em um dataframe, provavelmente decorrente de erros de digita√ß√£o ou de c√°lculo.

    ![Evidencia](./evidencias/curso_machine_learning_e_data_science/sec29/boxplot.png)

    Scatter plot que mostra a rela√ß√£o entre a idade e a pontua√ß√£o de uma pessoa.

    ![Evidencia](./evidencias/curso_machine_learning_e_data_science/sec29/scatter.png)

# üë®üèº‚Äçüéì Certificados

- Certificado do Curso **Machine Learning com Amazon AWS e SageMaker**

    ![Evidencia](./certificados/aws_e_sagemaker.jpg)