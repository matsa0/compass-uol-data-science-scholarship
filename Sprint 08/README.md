<h1 align="center">
    <strong>SPRINT 08</strong>
</h1>

# üîó V√≠deo - [Desafio Sprint 05](https://compasso-my.sharepoint.com/:v:/r/personal/matheus_azevedo_pb_compasso_com_br/Documents/Sprint8_Video_Desafio_Matheus_Azevedo.mp4?csf=1&web=1&nav=eyJyZWZlcnJhbEluZm8iOnsicmVmZXJyYWxBcHAiOiJPbmVEcml2ZUZvckJ1c2luZXNzIiwicmVmZXJyYWxBcHBQbGF0Zm9ybSI6IldlYiIsInJlZmVycmFsTW9kZSI6InZpZXciLCJyZWZlcnJhbFZpZXciOiJNeUZpbGVzTGlua0NvcHkifX0&e=qwdQA0)

# üìå Projeto 3

[Clique aqui para visualizar o c√≥digo do projeto](project03.ipynb)

## üìú Introdu√ß√£o

Muitas pessoas t√™m dificuldade em obter empr√©stimos devido a hist√≥ricos de cr√©dito insuficientes ou inexistentes, e
frequentemente s√£o exploradas por credores n√£o confi√°veis.

Nesta sprint, tinhamos como desafio desenvolver um modelo machine learning capaz de **prever a capacidade de pagamento dos clientes** e decidir se o empr√©stimo ser√° concedido. A ideia √© garantir que clientes com capacidade de pagamento **n√£o sejam rejeitados** e que os empr√©stimos oferecidos sejam adequados para capacitar seus clientes a terem sucesso.

Para isso, nos foi fornecido um conjunto de datasets com dados estat√≠sticos de aplica√ß√µes de empr√©stimo, hist√≥rico de pagamentos al√©m de cr√©ditos anteriores fornecidos por outras institui√ß√µes financeiras, que foram relatados ao bir√¥ de cr√©dito.

## üîé An√°lise Explorat√≥ria de Dados

Ap√≥s carregar os datasets e visualizar informa√ß√µes b√°sicas como n√∫mero de linhas e colunas, tipos de vari√°veis, porcentagem de valores nulos e duplicatas, eu parti para a an√°lise explorat√≥ria. 

A etapa de EDA √© feita com o objetivo de entender os dados trabalhados. Portanto, para cada dataset eu fiz uma an√°lise mais profunda das colunas presentes nos datasets.

Vou utilizar como exemplo o dataset principal, mas para todos os outros dataframes, as mesmas an√°lises foram aplicadas.

### An√°lise de valores nulos:

![Evidencia](./evidencias/eda/grafico_valores_nulos.png)

Para cada dataset eu plotei um gr√°fico de linha que exibe a **porcentagem de valores nulos para cada vari√°vel**. N√£o foi definido um threshold exato para exclus√£o de vari√°veis, mas features com 60% de valores nulos foram exclu√≠das e outras com 40% e at√© 50% foram analisadas para verificar se pode existir alguma import√¢ncia para o modelo. 

### Desbalanceamento

Desde o princ√≠pio foi verificado que a distribui√ß√£o dos dados era desbalanceada, evidenciada pela vari√°vel **TARGET**. Portanto, era mais um fator a se atentar durante toda a an√°lise.

![Evidencia](./evidencias/eda/distribui√ß√£o_variavel_alvo.png)

### An√°lise de features categ√≥ricas

Para melhor entendimento das colunas categ√≥ricas de cada conjunto de dados, eu plotei um subplot de histogramas. 

![Evidencia](./evidencias/eda/subplots_cat_vars.png)

Atrav√©s desses gr√°ficos foi poss√≠vel analisar muito mais facilmente a distribui√ß√£o de cada vari√°vel categ√≥rica, sendo poss√≠vel pensar em poss√≠veis manipula√ß√µes de tipos, categoriza√ß√µes, exclus√£o de vari√°veis e dentre outras conclus√µes.

### An√°lise de features num√©ricas

Para melhor entendimento das colunas num√©ricas de cada conjunto de dados, eu plotei um subplot de histogramas. 

![Evidencia](./evidencias/eda/subplots_num_vars.png)

Atrav√©s desses gr√°ficos foi poss√≠vel analisar muito mais facilmente a distribui√ß√£o de cada vari√°vel num√©rica, sendo poss√≠vel identificar desbalanceamento, distribui√ß√µes assim√©tricas, outliers e dentra outras conclus√µes.

## Conclus√£o

Ap√≥s uma detalhada an√°lise explorat√≥ria, baseada majoritariamente nas an√°lises acima, eu decidi utilizar para compor o dataset final os datasets: 
- **application_train**
- **bureau**
- **bureau_balance**
- **POS_CASH_balance**
- **instalments_payments**

Isso devido principalmente a complexidade de manipula√ß√£o dos datasets **previous_application** e **credit_card_balance**. Al√©m disso, os datasets escolhidos j√° possuem muitas informa√ß√µes relevantes para a an√°lise do risco de inadimpl√™ncia, n√£o sendo preciso aumentar a dimensionalidade do dataset final para englobar mais 2 datasets, o que deixaria o treinamento do modelo ainda mais complexo e demorado.

## ‚öôÔ∏è Modelagem

Ap√≥s a an√°lise explorat√≥ria de todos os datasets, chega o momento da modelagem dos dados. Nessa etapa eu englobei principalmente as agrega√ß√µes dos dados de acordo com a chave prim√°ria de cada dataset e os merges para alcan√ßar o dataset final.

Para minimizar a perda de informa√ß√µes de cr√©ditos relacionados, foi necess√°rio trabalhar com uma ordem espec√≠fica de merges, de forma a utilizar o melhor das chaves que estavamos trabalhando (**SK_ID_CURR** e **SK_ID_PREV**). 

Por exemplo, antes de agregar informa√ß√µes no dataset **bureau**, foi necess√°rio primeiro agregar informa√ß√µes no dataset **bureau_balance**. Isso porque a chave **SK_ID_BUREAU** √© √∫nica no dataset **bureau**, e se agruparmos por **SK_ID_CURR** (id do empr√©stimo) iremos perder muitas informa√ß√µes sobre os cr√©ditos relacionados. Portanto, agrupando primeiro pela chave **SK_ID_BUREAU** no dataset **bureau_balance**, conseguimos extrair o melhor dos dados nos dois datasets.

Esse mesmo cuidado foi tomado para realizar o merge com o restante dos datasets, deixando por √∫ltimo, o dataset principal (**application_train**).

O dataset final tinha o shape de **818298 rows √ó 58 columns**.

√â importante ressaltar foi tomado cuidado em n√£o imputar valores ou realizar qualquer etapa de pr√©-processamento antes de dividir o conjunto de dados final em treino e teste.

    X_train: (572808, 54)
    X_test: (245490, 54)
    y_train: (572808,)
    y_test: (245490,)

## üõ†Ô∏è Pr√©-processamento

No pr√©-processamento, uma ferramenta muito √∫til trabalhada foi o pipeline, da biblioteca sklearn. Ele facilita muito essa etapa, pois atrav√©s dele √© poss√≠vel organizar, padronizar e ter uma consist√™ncia do pr√©-processamento de dados

    #Transformer de vari√°veis num√©ricas
    num_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler())
    ])
Para vari√°veis num√©ricas utilizei um simples **imputador de m√©dia** para valores nulos e o **StandardScaler** para normaliza√ß√£o dos dados.

    #Transformer de vari√°veis categ√≥ricas
    cat_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OneHotEncoder(handle_unknown='ignore'))
    ])
Para vari√°veis categ√≥ricas, eu utilizei um simples **imputador de moda** e **One Hot Encoder** para transformar as categorias para num√©ricas.

Restante do pipeline:

    #Combina√ß√£o dos pr√©-processadores
    preprocessor = ColumnTransformer(
        transformers = [
            ('num', num_transformer, num_features),
            ('cat', cat_transformer, cat_features)
        ]
    )

    #Pipeline final
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor)
    ])

    #fit_transform em X_train e transform em X_test, de forma a evitar data leakage
    X_train_transformed = pipeline.fit_transform(X_train)
    X_test_transformed = pipeline.transform(X_test)

## ü§ñ Treinamento e avalia√ß√µes dos modelos 

Na etapa de treinamento dos modelos, pela conhecida demora na execu√ß√£o de algoritmos de SVM, eu escolhi me basear em algoritmos de **regress√£o log√≠stica** e **√°rvores de decis√£o**.

Portanto, utilzei 2 algoritmos de cada abordagem para poder avaliar melhor diferentes m√©todos. Na regress√£o log√≠stica, utilizei o **LogisticRegression com regulariza√ß√µes** e o **SGDClassifier**, que √© baseada em otimiza√ß√£o de gradient. J√° para √°rvores de decis√£o, resolvi utilizar o Catboost e o XGBoost. Para algortimos baseados em √°rvores, eu utilizei o **CatBoostClassifier** e o **XGBoostClassifier**.

### Primeiro Resultado

![Evidencia](./evidencias/treinamento/primeiros_resultados.png)

Nos primeiros treinamentos, os algoritmos baseados em √°rvore se destacaram sobre os algoritmos de regress√£o log√≠stica. Em especial, o **CatBoostClassifier** que teve um balan√ßo melhor das m√©tricas de **recall**, **f1-score** e **AUC** em compara√ß√£o com os demais algoritmos.

### Tunning

Como os resutados ainda podem melhorar, principalmente para m√©tricas com recall e f1-score, resolvi fazer um Tuning de hiperpar√¢metros utilizando o Optuna a fim de descobrir os melhores par√¢metros **maximizando a m√©trica f1-score**.

Ap√≥s realizar o Tuning e obter os melhores par√¢metros, eu re-treinei o algoritmo CatBoostClassifier e obtive as seguintes novas m√©tricas:

![Evidencia](./evidencias/treinamento/tuning.png)

√â not√°vel que todas as m√©tricas melhoraram bastante, com destaque ao F1-Score e o AUC, que dizem bastante sobre o nosso modelo.

### Matriz de confus√£o

![Evidencia](./evidencias/treinamento/matriz_de_confusao.png)

Atrav√©s da matriz de confus√£o √© poss√≠vel observar que o modelo prev√™ muito bem bons pagadores. Al√©m disso, considerando o tamanho da base de dados, os erros de predi√ß√£o em inadimpl√™ncia quando √© bom pagador e vice-versa, s√£o bem poucos. Al√©m disso, considerando o desbalanceamento da vari√°vel alvo, o modelo tamb√©m est√° prevendo inadimpl√™ncia de forma satisfat√≥ria. 

### Curva ROC

![Evidencia](./evidencias/treinamento/curva_roc.png)

A curva ROC nos mostra que a taxa de Verdadeiro Postivo √© muito alta, ent√£o, ao escolher um cliente aleat√≥rio que √© inadimplente e outro que n√£o √©, h√° 93% de chance do modelo classificar corretamente o inadimplente com uma pontua√ß√£o maior.

Na pr√°tica, o modelo est√° fazendo boas previs√µes, minimizando falsos negativos **(aprovar inadimplentes)** e falsos positivos **(rejeitar bons pagadores)**.

## üß† Conclus√£o

O modelo de **CatBoostClassifier** apresentou um √≥timo desempenho na previs√£o da capacidade de pagamento dos clientes, destacando-se em rela√ß√£o aos outros algoritmos testados. 

O modelo conseguiu equilibrar **precis√£o e recall**, indicando que est√° prevendo bem os clientes inadimplentes sem comprometer excessivamente os bons pagadores.

Houve tamb√©m uma boa separa√ß√£o entre pagadores e inadimplentes, e apesar do desbalanceamento dos dados, o modelo consegue capturar inadimplentes de forma satisfat√≥ria.
