<h1 align="center">
    <strong>SPRINT 06</strong>
</h1>

# üîó V√≠deo - [Desafio Sprint 06](https://compasso-my.sharepoint.com/:v:/r/personal/matheus_azevedo_pb_compasso_com_br/Documents/Sprint6_Video_Desafio_MatheusAzevedo.mp4?csf=1&web=1&e=m1w56h)

# üìù Exerc√≠cios

### üß† Curso: Forma√ß√£o Processamento de Linguagem Natural, LLMs e Gen AI

- Diret√≥rio com os notebooks trabalhados: [Clique aqui](./exercicios/curso_formacao_processamento_nlp/)

### üß† Curso: Face Recognition with Machine Learning + Deploy Flask App

- Diret√≥rio com os notebooks trabalhados: [Clique aqui](./exercicios/curso_face_recognition/)

### üß† Curso: MLOps: Implanta√ß√£o e Opera√ß√£o de Modelos de Machine Learning

- Diret√≥rio com os notebooks trabalhados: [Clique aqui](./exercicios/curso_mlops/)

# üîé Evid√™ncias

### üß† Curso: Forma√ß√£o Processamento de Linguagem Natural, LLMs e Gen AI

- **Se√ß√£o 2: Fundamentos de Processamento de Linguagem Natural**<br>
Na se√ß√£o 2 aprendi sobre conceitos introdut√≥rios que s√£o muito utilizados e importantes para o entendimento de Processamento de Linguagem Natural, como por exemplo:
    - **Corpus:**<br>√â um conjunto de texto estrutrado em linguagem natural.
    - **Annotations:**<br>Elementos espec√≠ficos no texto que dependem do dom√≠nio em que se trabalha
    - **Tokenization:**<br>Processo de separar a senten√ßa em suas partes: palavras, pontos, s√≠mbolos, stop words, alphanumeric etc.
    - **Parts-of-Speech Tagging (POS):**<br>Adiciona tags a cada token, como por exemplo, se √© verbo, substantivo, adjetivo etc.
    - **Stop Words:**<br>S√£o palavras que n√£o s√£o relevantes para o resultado de uma busca, e por isso podem ser removidas do texto. Ou seja, n√£o possuem emo√ß√£o, n√£o alteram o sentido do texto e n√£o t√™m valor sem√¢ntico.

    Para usar um modelo estat√≠stico ou de deep learning em NLP, √© necess√°rio transformar o texto em uma informa√ß√£o num√©rica, mais especificamente um **vetor**. **Word Embeddings** s√£o representa√ß√µes computacionais de texto.

- **Se√ß√£o 3: NLP com Spacy**<br>
Na se√ß√£o 3 trabalhei com o `Spacy`, que √© uma biblioteca de Linguagem de Processamento Natural Open-Source que nos permite fazer download de **modelos pr√©-treinados em portugu√™s**. Esses modelos permitem que possamos processar e extrair informa√ß√µes relevantes de textos em portugu√™s de maneira eficiente.

    Nesse m√≥dulo trabalhei com **Tokens**, **Pos-Tagging**, **Stop-Words**, **Similaridade**, **Pipelines**, entre outros.

    Os `Tokens` representam a menor unidade de texto que um modelo NLP pode processar. E o processo de **tokeniza√ß√£o** consiste em  dividir um texto em palavras, pontua√ß√µes ou at√© mesmo subpalavras

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec3/tokenization.png)

    A `similaridade` mede o qu√£o semelhantes s√£o duas palavras, frases ou documentos. A m√©trica utilizada √© o **cosseno**, um valor entre **0 e 1**, ou seja, quanto mais pr√≥ximo de 1 mais similaridade, quanto mais pr√≥ximo de 0 menos similaridade.

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec3/similarity.png)

    `Displacy` √© um m√≥dulo do Spacy para visualiza√ß√£o.

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec3/displacy.png)

- **Se√ß√£o 4: NLP com NLTK**<br>
Na se√ß√£o 4 vi sobre o `NLTK` que √© uma biblioteca Python de Processamento de Linguagem Natural. Ele √© similar ao Spacy, por√©m mais **poderoso** e **vers√°til**. Al√©m disso, tamb√©m exige mais configura√ß√£o manual.

    √â necess√°rio a instala√ß√£o de alguns pacotes para a sua utiliza√ß√£o:

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec4/packages.png)

    A biblioteca NLTK realmente tem funcionalidades muito parecidas com a SpaCy. Algumas funcionalidades que eu achei interessantes de se estudar foram a **produ√ß√£o de m√©tricas** e o **Stemming**.

    Uma simples produ√ß√£o de m√©tricas √© a contagem de palavras mais comuns em um conjunto de tokens. Isso √© feito pelo **nltk.FreqDist** e **most_common**.

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec4/metrics.png)

    `Stemming` √© o processo de analisar cada palavra reduzi-la √† sua raiz(seu **significado**). Uma caracter√≠stica dessa t√©cnica √© que ela pode reduzir a palavra a uma outra gramaticalmente **incorreta**, por√©m ainda com **valor** para nossa an√°lise. 

    Existem 3 tipos de Stemming:
    - Porter: mais comum e mais antigo

        ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec4/stemming_porter.png)

    - Snowball: melhor performance computacional

        ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec4/stemming_snow.png)

    - Lancaster: mais agressivo. Resultados as vezes n√£o intuitivos

        ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec4/stemming_lancaster.png)

- **Se√ß√£o 6: Machine Learning e Deep Learning para NLP na Pr√°tica**<br>
    
    Nesta se√ß√£o desenvolvemos o treinamento de uma `Rede Neural`. Foi interessante para relembrar alguns importantes conceitos relacionados, como por exemplo:

    Hiperpar√¢metros:

    - **loss:** fun√ß√£o de perda para avalia√ß√£o do erro. ex: mean_squared_error
    - **optimizer:** otimizador de ajuste de pesos da rede. stochastic gradient descnet
    - **metrics:** a m√©trica que a rede vai utilizar para avaliar a performance

    As fun√ß√µes de ativa√ß√£o como **relu** e **sigmoid** que s√£o fundamentais para redes neurais, pois introduzem n√£o-linearidade, permitindo que a rede aprenda padr√µes complexos.

    Por fim, o **dropout**, que √© uma t√©cinca que desativa aleatoriamente neur√¥nios durante o treinamento, a fim de reduzir o overfitting e melhorar a capacidade de aprendizado e generaliza√ß√£o da rede. 

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec6/parameters_neural_network.png)

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec6/fit_neural_network.png)

    AS m√©tricas evidenciam uma **√≥tima performance** do modelo para prever se uma mensagem √© spam ou veridica.

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec6/metrics.png)

    Foi interessante tamb√©m colocar em pr√°tica o uso de `Embeddings`. Treinar uma rede neural com Embedding na pr√°tica √© transformar as palavras em vetores densos de baixa dimens√£o que capturam significado e contexto.

    O uso de Embeddings proporciona as seguintes vantagens:
    - Redu√ß√£o de dimensionalidade, pois cada palavra √© um vetor de poucas dimens√µes
    - A captura do significado sem√¢ntico da palavra
    - Aprendicado contextual pela rede

    Antes de aplicar embeddings no modelo, √© necess√°rio gerar os `Tokens` do texto. Posteriormente, cada token √© substitu√≠do por um n√∫mero que representa sua posi√ß√£o no vocabul√°rio, isso pois o embedding requer n√∫meros inteiros como entrada e as palavras ir√£o receber identificadores √∫nicos. Para finalizar, o `Padding` padroniza o tamanho das sequ√™ncias.

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec6/tokenization.png)

    O modelo com o Embedding:

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec6/modelo_embedding.png)

- **Se√ß√£o 7: An√°lise de Sentimentos**<br>
Na se√ß√£o 7 √© trabalhado a √°rea de `An√°lise de Sentimentos`, muito importante em v√°rios modelos de neg√≥cio, geralmente para avaliar a **percep√ß√£o dos usu√°rios** sobre um produto, monitorar redes sociais, marcas e realizar uma pesquisa de mercado no geral. √â interessante ressaltar que o **contexto √© muito importante**, pois ocorr√™ncias de ironia, sarcasmo e ambiguidade s√£o tarefas desafiadoras ao analisar sentimentos.

    Uma forma de se trabalhar com an√°lise de sentimentos √© com o `LSTM`, sigla para `Long Short-Term Memory`, ou seja, mem√≥ria de curto e longo prazo. √â um tipo de rede neural recorrente (RNN) que consegue **memorizar informa√ß√µes por longos per√≠odos de tempo**.

    √â interessante destacar o pr√©-processamento realizado para a utiliza√ß√£o do LSTM, os coment√°rios no c√≥digo j√° explicam para que cada funcionalidade serve:

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec7/pre_processing_lstm.png)

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec7/pre_processing_lstm2.png)

    Outra interessante alternativa, √© realizar a an√°lise de sentimentos com o **Vader**.  Vader trabalha com uma an√°lise de sentimentos **baseado em regras**, portanto √© interessante saber alguns conceitos:

    - `Compound`: m√©trica que representa o **sentimento geral** de um texto. √â um valor real entre -1 (muitonegativo) e 1 (muito positivo).
    - `Polarity`: Mede o grau de **positividade ou negatividade** de um texto. Valor real entre -1 (muito negativo) e 1 (muito positivo).
    - `Subjectivity`: O quanto a senten√ßa se refere a **opini√£o**, **julgamento** ou **emo√ß√£o**. Valor real entre 0 e 1, quando mais pr√≥ximo de 1, mais subjetivo.

    Exemplos:

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec7/vader.png) 

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec7/vader2.png) 

- **Se√ß√£o 8: Transformers, GPT (do ChatGPT) e Bert**<br>
Na se√ß√£o 8, trabalhei com os `Transformers`, que s√£o um tipo de arquitetura de rede neural baseada em **aten√ß√£o** que revolucionou o processamento de linguagem natural (NLP). Sua principal funcionalidade √© **processar** e **entender** **sequ√™ncias de texto** de maneira eficiente, usando o mecanismo de aten√ß√£o para capturar **depend√™ncias** entre palavras. Isso permite que ele traduza, gere, resuma e classifique textos com alta precis√£o.


    As principais caracter√≠sticas s√£o:
    - `Self-Attention`: tem a fun√ß√£o de descobrir a **rela√ß√£o entre as palavras**. O transformer calcula a representa√ß√£o e a rela√ß√£o de cada palavra.
    - `Multi-Attention`: Em vez de um processo Attention, m√∫ltiplos processos s√£o processados e o resultado √© somado.
    - `Processamento Paralelo`: Diferente de RNNs/LSTMs, os Transformers** n√£o precisam processar palavras em sequ√™ncia**, tornando-os muito mais r√°pidos.
    - `Transfer Learning`: Modelos como BERT e GPT podem ser **pr√©-treinados em grandes quantidades de dados** e depois **ajustados para tarefas espec√≠ficas**.

    A **arquitetura** do Transformer √© dividida em duas partes principais:
    - `Encoder (Codificador)`: Extrai **significado** do texto de entrada.
    - `Decoder (Decodificador)`: Gera a **sa√≠da baseada no contexto aprendido** pelo encoder.

    Outro conceito muito importante que √© basedo em **transformers** √© o `BERT`, um modelo de linguagem capaz de entender o **contexto completo de uma palavra dentro de uma frase**, considerando tanto as palavras da esquerda, quanto da direita.

    O BERT possui v√°rias variantes com diversas caract√©ricas como serem mais leves, ou mais r√°pidas ou menores, entre outras.

    Nesta se√ß√£o utilizei da plataforma colaborativa `HuggingFace` que possui uma gama muito vasta de modelos capazes de lidar com processamento de texto, a√∫dio e imagem. A plataforma possui uma lib chamada `Transformers`, que possui v√°rias funcionalidades √∫teis, como por exemplo, o `Pipeline`. 
    
    Um **pipeline** √© uma abstra√ß√£o que **encapsula todo o fluxo de entrada e sa√≠da para uma tarefa espec√≠fica de PLN**. Ele cuida de toda a pr√©-processamento, infer√™ncia e p√≥s-processamento, permitindo que voc√™ use modelos sem precisar lidar diretamente com as complexidades da arquitetura.

    Exemplo de um pipeline **Q&A(Question Answering)**:

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec8/pipeline_q&a.png)

    Outro pipeline interessante √© o de **preenchimento de lacunas(Fill Mask)**. Ele d√° algumas sugest√µes de palavras para completar a lacuna com um score para cada uma:

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec8/pipeline_fillmask.png)

    Essas sugest√µes de onde jogar o lixo n√£o foram muito boas... üòÖ

    Outra plataforma trabalhada nessa se√ß√£o foi a **OpenAI** para utiliza√ß√£o do modelo `GPT-3`. Por√©m, como a OpenAI n√£o fornece mais free trial, n√£o √© poss√≠vel utilizar os modelos GPT.

- **Se√ß√£o 9: Modelagem de T√≥picos com Bert**<br>
Na se√ß√£o 9 aprendi sobre **modelagem de t√≥picos** utilizando o `BERT`, conceituado na se√ß√£o anterior. A modelagem de t√≥picos √© uma t√©cnica usada para descobrir automaticamente os **temas principais** dentro de um grande conjunto de textos.

    O `BERTopic` √© um modelo avan√ßado de modelagem de t√≥picos que combina **embeddings** de linguagem (como BERT) com algoritmos de clusteriza√ß√£o para identificar t√≥picos de maneira mais precisa.

    Cria√ß√£o do modelo BERTopic:

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec9/bertopic.png)

    T√≥picos encontrados:

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec9/frequency_topics.png)

    Visualiza√ß√£o dos scores de 6 palavras dos top 8 t√≥picos:

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec9/topic_words_score.png) 

- **Se√ß√£o 10: NLP com Spark**<br>
Na se√ß√£o 10 trablhei com o `Spark` utilizando um cluster no **Databricks Community**. Databricks √© uma plataforma otimizada para utiliza√ß√£o de Spark, onde NLP pode ser utilizado usando Deep Learning e embeddings de BERT, RoBERTa, Word2Vec, etc.

    Para estudar um pouco sobre como funcionava NLP dentro do Spark, o professor utilizou a mesma base spam.csv de outras se√ß√µes.

    Transforma√ß√£o da vari√°vel **Category**(semelhante ao labelencoder):

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec10/category_transform.png)

    Tokeniza√ß√£o da vari√°vel **Message**:

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec10/message_transform.png)

    Utilizando o `Word2Vec` para representar a vari√°vel **Message** vetorialmente.

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec10/w2v_vetorial.png)

    Ap√≥s o treinamento do RandomForestClassifier, as previs√µes foram geradas. √â interessante observar que o Spark cria 3 colunas adicionais: **rawPrediction**, **probability** e **prediction**.

    - `rawPrediction`: Soma das probabilidades dos votos de **todas as √°rvores do Random Forest**. Representa a contagem ponderada dos votos para cada classe.
    - `probability`:  **Normaliza√ß√£o** da rawPrediction, convertendo-a em uma distribui√ß√£o de probabilidades entre 0 e 1 para cada classe.
    - `prediction`: Classe **final prevista**, baseada na classe com maior probabilidade em probability.

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec10/previsions.png)


### üß† Curso: Face Recognition with Machine Learning + Deploy Flask App

- **Se√ß√£o 2: Processamento de imagem com OpenCV**<br>
Na se√ß√£o 2 aprendi a trabalhar com o `OpenCV`, que basicamente √© uma biblioteca voltada para **vis√£o computacional** e **processamento de imagens**.

    √â importante entender como uma m√°quina entende uma imagem.
    Uma imagem digital √© representada como uma **matriz de pixels**, onde cada pixel cont√©m informa√ß√µes sobre a **intensidade de cor**. Em imagens **em escala de cinza**:

    - Valores baixos (pr√≥ximos de 0) representam tons mais escuros (preto).
    - Valores altos (pr√≥ximos de 255) representam tons mais claros (branco).

    √â poss√≠vel ver isso na pr√°tica:

    ![Evidencia](./evidencias/curso_face_recognition/sec2/colors256pixels.png)

    Em **imagens coloridas**, cada pixel cont√©m tr√™s canais conhecidos como **RGB(Red, Green, Blue)**. Por√©m, o OpenCV l√™ imagens no formato **BGR (Blue, Green, Red)**. O OpenCV trabalha com a ordem dos canais invertidas por quest√µes hist√≥ricas e de efici√™ncia, mas como a convers√£o para RGB √© f√°cil, isso n√£o impacta o processamento de imagens. √â poss√≠vel visualizar a difere√ßa no exemplo abaixo:

    ![Evidencia](./evidencias/curso_face_recognition/sec2/imgs_bgr_rgb.png)

    Algo interessante de visualizar, √© a separa√ß√£o dos canais **BGR**, onde √© poss√≠vel ver como a imagem reconhece cada cor. As partes com mais contraste(**mais amarelas**) de cada imagem representa a cor do respectivo canal.

    ![Evidencia](./evidencias/curso_face_recognition/sec2/split_channels.png)

    Outro conceito explorado foi o de `Face Detection` utilizando o `Haar Cascade Classifier`, um modelo pr√©-treinado que usa um arquivo XML especializado na **detec√ß√£o de rostos frontais**. 

    Os passos para utilizar o Haar s√£o:
    - Convers√£o para Escala de Cinza
    - Carregamento do Classificador Pr√©-Treinado
    - Detec√ß√£o de Objetos na Imagem
    - Desenho das detec√ß√µes

    ![Evidencia](./evidencias/curso_face_recognition/sec2/face_detection.png)


- **Se√ß√£o 3: Desenvolvendo um modelo de Reconhecimento Facial com Machine Learning**<br>

    - `Crop Faces`: O processo de crop face (recorte de rosto) envolve detectar e extrair a regi√£o da face em uma imagem, para utiliza√ß√£o em aplica√ß√µes de reconhecimento facial.
    
        Antes:<br>
            ![Evidencia](./evidencias/curso_face_recognition/sec3/before_crop_face.png)

        Depois:<br>
            ![Evidencia](./evidencias/curso_face_recognition/sec3/after_crop_face.png)

    - `Eigenfaces`: Eigenfaces s√£o um conjunto de autovetores de uma matriz de covari√¢ncia formada por **imagens de faces (rostos)**. Esta √© uma t√©cnica que busca representar **padr√µes encontrados em imagens** de rostos utilizando o m√©todo PCA (An√°lise de Componentes Principais)*.

        Com Eigenfaces, pode-se realizar o reconhecimento de rostos, ou seja, √© poss√≠vel dizer** a quem determinado rosto pertence**, baseado num banco de dados sobre esta pessoa previamente cadastrado.

        √â importante realizar a extra√ß√£o de caracter√≠sticas de imagens de rostos usando a t√©cnica de Eigenfaces, que √© baseada em An√°lise de Componentes Principais (PCA). A `mean face` √© calculada como a m√©dia de todas as imagens do dataset. Essa face m√©dia representa o **"rosto t√≠pico"** do conjunto de dados.

        Exemplo de mean face:

        ![Evidencia](./evidencias/curso_face_recognition/sec3/mean_face.png)
        
        A Eigenface s√£o os componentes principais extra√≠dos das imagens faciais que capturam padr√µes de varia√ß√£o entre os rostos, permitindo representar faces de forma compacta e eficiente.

        ![Evidencia](./evidencias/curso_face_recognition/sec3/eigenface.png)
    
    - `Pipeline`: Em v√°rias partes dessa se√ß√£o, foram realizados treinamentos de modelos e esses foram salvos atrav√©s da biblioteca **pickle**. Um pipeline √© criado com o objetivo de classificar o g√™nero de um rosto presente em uma imagem utilizando 3 modelos salvos.

    - `Classificador Haar Cascade`: Detecta rostos na imagem convertida para escala de cinza. Retorna as coordenadas dos rostos detectados.

    - `Modelo PCA`: Cont√©m os componentes principais (Eigenfaces) e a face m√©dia do conjunto de treinamento. O rosto detectado √© transformado para o espa√ßo PCA, reduzindo a dimensionalidade dos dados antes da classifica√ß√£o.

    - `Modelo SVM`: Um classificador SVM treinado para distinguir entre os g√™neros masculino e feminino. Recebe a representa√ß√£o do rosto no espa√ßo PCA e retorna a previs√£o do g√™nero, junto com a pontua√ß√£o de confian√ßa.

        ![Evidencia](./evidencias/curso_face_recognition/sec3/result_pipeline.png)


### üß† Curso: MLOps: Implanta√ß√£o e Opera√ß√£o de Modelos de Machine Learning

- **Se√ß√£o 1: Introdu√ß√£o**<br>
Criar um bom modelo machine learning j√° √© um grande desafio, por√©m **implant√°-lo** e **mant√™-lo em produ√ß√£o** √© um desafio ainda maior. 

    √â interessante ressaltar que o ciclo de vida de um **modelo** √© diferente do ciclo de vida do **c√≥digo** e do ciclo de vida dos **dados**. Modelos s√£o tempor√°rios, isso pois eles dependem de dados e dados mudam com o tempo.

    Para falar de `MLOps`, primeiro temos que contextualizar o `DevOps`. DevOps √© a integra√ß√£o de **Desenvolvimento** e **Opera√ß√µes**, de forma que as duas √°reas possam trabalhar em conjunto para automatizar o processo de implanta√ß√£o com entrega cont√≠nua. O MLOps nada mais √© que o **DevOps para Machine Learning**, uma pr√°tica que combina o desenvolvimento de aplica√ß√µes de machine learning com a implanta√ß√£o e opera√ß√µes do sistema. 

    Os **n√≠veis** MLOPS:
    - `N√≠vel 0`: Manual
    - `N√≠vel 1`: Automatiza√ß√£o do pipeline de Machine Learning
    - `N√≠vel 2`: Automatiza√ß√£o da Integra√ß√£o Cont√≠nua e da Entrega Cont√≠nua

- **Se√ß√£o 2: Apresentando MLFlow**<br>
Nesta se√ß√£o aprendi sobre o `MLFlow`, uma plataforma de c√≥digo aberto para **gerenciamento do ciclo de vida** de modelos Machine Learning. Ele facilita o gerenciamento do fluxo de trabalho para treinamento, rastreamento de experimentos e produ√ß√£o de modelos. 

- **Se√ß√£o 4: Criando e Registrando Modelos**<br>
Cria√ß√£o do meu **primeiro experimento MLFlow** utilizando um modelo **Naive Bayes**:

    ![Evidencia](./evidencias/curso_mlops/sec4/first_experiment.png)

    √â poss√≠vel visualizar o modelo criado no experimento atrav√©s do diret√≥rio `mlruns`, onde voc√™ navega pelas pastas e encontra um diret√≥rio com a **chave do modelo treinado**. O diret√≥rio cont√©m informa√ß√µes sobre **artefatos, m√©tricas, pat√¢metros e tags do modelo criado**. No exemplo abaixo, a acur√°cia que foi registrada no experimento est√° dentro da pasta metrics.

    ![Evidencia](./evidencias/curso_mlops/sec4/mlruns.png)

    No seu ambiente virtual, digite o comando `mlflow ui` e uma interface local do **mlflow** ir√° rodar. L√° √© poss√≠vel ver todas as informa√ß√µes a respeito dos seus experimentos criados, metricas, par√¢metros, entre outros.

    ![Evidencia](./evidencias/curso_mlops/sec4/mlflow_ui.png)

    Experimento com mais m√©tricas:

    ![Evidencia](./evidencias/curso_mlops/sec4/metrics_mlflow_ui.png)

    Os artefatos incluem o modelo salvo, o enviroment, gr√°ficos gerados e podem conter outros tipos de artefatos.

    ![Evidencia](./evidencias/curso_mlops/sec4/artifacts.png)

    Outra funcionalidade interessante de se explorar ao treinar modelos com MLFlow, √© testar **diferentes combina√ß√µes** de **hiperpar√¢metros** e visualizar detalhadamente os resultados obtidos.

    No exemplo abaixo, configurei um experimento de **Deep Learning com Keras**, onde selecionei alguns hiperpar√¢metros para serem testados automaticamente:

    ![Evidencia](./evidencias/curso_mlops/sec4/hyperparametes_dl.png)

    Os diversos modelos criados no experimento MLFlow:

    ![Evidencia](./evidencias/curso_mlops/sec4/dl_models.png)

    Ao clicar em qualquer um desses modelos, √© poss√≠vel visualizar informa√ß√µes detalhadas, como os hiperpar√¢metros utilizados, m√©tricas obtidas, tags e outros detalhes importantes para an√°lise e compara√ß√£o.

- **Se√ß√£o 5: Servindo Modelos on premise / local**<br>
Na se√ß√£o 5 aprendi como servir modelos, o que na pr√°tica significa **disponibilizar modelos** de machine learning **localmente** em vez de utilizar servi√ßos em nuvem.

    √â interessante disponibilizar modelos ap√≥s a etapa de experimentos, onde voc√™ encontrou o melhor modelo e pode permitir que outras aplica√ß√µes, usu√°rios ou sistemas possam aproveitar do desempenho do modelo criado para tomar decis√µes ou gerar insights.

    Com o comando `mlflow models serve --env-manager local -m runs:/8f0ce2208a5a4978aad2283a025d2c76/RFmodel -p 2345` foi poss√≠vel servir o modelo localmente, sendo **8f0ce2208a5a4978aad2283a025d2c76** o ID do modelo, **RFmodel** seu nome, e **2345** a porta.

    ![Evidencia](./evidencias/curso_mlops/sec5/request.png)

    Com o modelo servido locamente, √© poss√≠vel fazer fazer uma requisi√ß√£o **HTTP POST**  enviando dados no formato JSON para **obter previs√µes de um modelo** de machine learning que est√° sendo servido.

# üë®üèº‚Äçüéì Certificados

### üß† Curso: Forma√ß√£o Processamento de Linguagem Natural, LLMs e Gen AI

![Certificado](./certificados/curso_formacao_processamento_nlp/certificado_curso_processamento_nlp.jpg)

### üß† Curso: Face Recognition with Machine Learning + Deploy Flask App

### üß† Curso: MLOps: Implanta√ß√£o e Opera√ß√£o de Modelos de Machine Learning

![Certificado](./certificados/curso_mlops/certificado_curso_mlops.jpg)