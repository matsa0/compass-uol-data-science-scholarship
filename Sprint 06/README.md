<h1 align="center">
    <strong>SPRINT 06</strong>
</h1>

# üîó V√≠deo - [Desafio Sprint 06]

# üìù Exerc√≠cios

### üß† Curso: Forma√ß√£o Processamento de Linguagem Natural, LLMs e Gen AI

- Diret√≥rio com os notebooks trabalhados: [Clique aqui](./exercicios/curso_formacao_processamento_nlp/)

### üß† Curso: Face Recognition with Machine Learning + Deploy Flask App

### üß† Curso: MLOps: Implanta√ß√£o e Opera√ß√£o de Modelos de Machine Learning

# üîé Evid√™ncias

### üß† Curso: Forma√ß√£o Processamento de Linguagem Natural, LLMs e Gen AI

- **Se√ß√£o 2: Fundamentos de Processamento de Linguagem Natural**<br>
Na se√ß√£o 2 aprendi sobre conceitos introdut√≥rios que s√£o muito utilizados e importantes para o entendimento de Processamento de Linguagem Natural, como por exemplo:
    - **Corpus:**<br>√â um conjunto de texto estrutrado em linguagem natural.
    - **Annotations:**<br>Elementos espec√≠ficos no texto que dependem do dom√≠nio em que se trabalha
    - **Tokenization:**<br>Processo de separar a senten√ßa em suas partes: palavras, pontos, s√≠mbolos, stop words, alphanumeric etc.
    - **Parts-of-Speech Tagging (POS):**<br>Adiciona tags a cada token, como por exemplo, se √© verbo, substantivo, adjetivo etc.
    - **Stop Words:**<br>S√£o palavras que n√£o s√£o relevantes para o resultado de uma busca, e por isso podem ser removidas do texto. Ou seja, n√£o possuem emo√ß√£o, n√£o alteram o sentido do texto e n√£o t√™m valor sem√¢ntico.

    Para usar um modelo estat√≠stico ou de deep learning em NLP, √© necess√°rio transformar o texto em uma informa√ß√£o num√©rica, mais especificamente um **vetor**. **Word Embeddings** s√£o representa√ß√µes computacionais de texto.

- **Se√ß√£o 3: NLP com Spacy**<br>
Na se√ß√£o 3 trabalhei com o `Spacy`, que √© uma biblioteca de Linguagem de Processamento Natural Open-Source que nos permite fazer download de **modelos pr√©-treinados em portugu√™s**. Esses modelos permitem que possamos processar e extrair informa√ß√µes relevantes de textos em portugu√™s de maneira eficiente.

    Nesse m√≥dulo trabalhei com **Tokens**, **Pos-Tagging**, **Stop-Words**, **Similaridade**, **Pipelines**, entre outros.

    Os `Tokens` representam a menor unidade de texto que um modelo NLP pode processar. E o processo de **tokeniza√ß√£o** consiste em  dividir um texto em palavras, pontua√ß√µes ou at√© mesmo subpalavras

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec3/tokenization.png)

    A `similaridade` mede o qu√£o semelhantes s√£o duas palavras, frases ou documentos. A m√©trica utilizada √© o **cosseno**, um valor entre **0 e 1**, ou seja, quanto mais pr√≥ximo de 1 mais similaridade, quanto mais pr√≥ximo de 0 menos similaridade.

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec3/similarity.png)

    `Displacy` √© um m√≥dulo do Spacy para visualiza√ß√£o.

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec3/displacy.png)

- **Se√ß√£o 4: NLP com NLTK**<br>
Na se√ß√£o 4 vi sobre o `NLTK` que √© uma biblioteca Python de Processamento de Linguagem Natural. Ele √© similar ao Spacy, por√©m mais **poderoso** e **vers√°til**. Al√©m disso, tamb√©m exige mais configura√ß√£o manual.

    √â necess√°rio a instala√ß√£o de alguns pacotes para a sua utiliza√ß√£o:

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec4/packages.png)

    A biblioteca NLTK realmente tem funcionalidades muito parecidas com a SpaCy. Algumas funcionalidades que eu achei interessantes de se estudar foram a **produ√ß√£o de m√©tricas** e o **Stemming**.

    Uma simples produ√ß√£o de m√©tricas √© a contagem de palavras mais comuns em um conjunto de tokens. Isso √© feito pelo **nltk.FreqDist** e **most_common**.

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec4/metrics.png)

    `Stemming` √© o processo de analisar cada palavra reduzi-la √† sua raiz(seu **significado**). Uma caracter√≠stica dessa t√©cnica √© que ela pode reduzir a palavra a uma outra gramaticalmente **incorreta**, por√©m ainda com **valor** para nossa an√°lise. 

    Existem 3 tipos de Stemming:
    - Porter: mais comum e mais antigo

        ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec4/stemming_porter.png)

    - Snowball: melhor performance computacional

        ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec4/stemming_snow.png)

    - Lancaster: mais agressivo. Resultados as vezes n√£o intuitivos

        ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec4/stemming_lancaster.png)

- **Se√ß√£o 6: Machine Learning e Deep Learning para NLP na Pr√°tica**<br>
    

    Nesta se√ß√£o desenvolvemos o treinamento de uma `Rede Neural`. Foi interessante para relembrar alguns importantes conceitos relacionados, como por exemplo:

    Hiperpar√¢metros:

    - **loss:** fun√ß√£o de perda para avalia√ß√£o do erro. ex: mean_squared_error
    - **optimizer:** otimizador de ajuste de pesos da rede. stochastic gradient descnet
    - **metrics:** a m√©trica que a rede vai utilizar para avaliar a performance

    As fun√ß√µes de ativa√ß√£o como **relu** e **sigmoid** que s√£o fundamentais para redes neurais, pois introduzem n√£o-linearidade, permitindo que a rede aprenda padr√µes complexos.

    Por fim, o **dropout**, que √© uma t√©cinca que desativa aleatoriamente neur√¥nios durante o treinamento, a fim de reduzir o overfitting e melhorar a capacidade de aprendizado e generaliza√ß√£o da rede. 

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec6/parameters_neural_network.png)

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec6/fit_neural_network.png)

    AS m√©tricas evidenciam uma **√≥tima performance** do modelo para prever se uma mensagem √© spam ou veridica.

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec6/metrics.png)

    Foi interessante tamb√©m colocar em pr√°tica o uso de `Embeddings`. Treinar uma rede neural com Embedding na pr√°tica √© transformar as palavras em vetores densos de baixa dimens√£o que capturam significado e contexto.

    O uso de Embeddings proporciona as seguintes vantagens:
    - Redu√ß√£o de dimensionalidade, pois cada palavra √© um vetor de poucas dimens√µes
    - A captura do significado sem√¢ntico da palavra
    - Aprendicado contextual pela rede

    Antes de aplicar embeddings no modelo, √© necess√°rio gerar os `Tokens` do texto. Posteriormente, cada token √© substitu√≠do por um n√∫mero que representa sua posi√ß√£o no vocabul√°rio, isso pois o embedding requer n√∫meros inteiros como entrada e as palavras ir√£o receber identificadores √∫nicos. Para finalizar, o `Padding` padroniza o tamanho das sequ√™ncias.

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec6/tokenization.png)

    O modelo com o Embedding:

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec6/modelo_embedding.png)

- **Se√ß√£o 7: An√°lise de Sentimentos**<br>
Na se√ß√£o 7 √© trabalhado a √°rea de `An√°lise de Sentimentos`, muito importante em v√°rios modelos de neg√≥cio, geralmente para avaliar a **percep√ß√£o dos usu√°rios** sobre um produto, monitorar redes sociais, marcas e realizar uma pesquisa de mercado no geral. √â interessante ressaltar que o **contexto √© muito importante**, pois ocorr√™ncias de ironia, sarcasmo e ambiguidade s√£o tarefas desafiadoras ao analisar sentimentos.

    Uma forma de se trabalhar com an√°lise de sentimentos √© com o `LSTM`, sigla para `Long Short-Term Memory`, ou seja, mem√≥ria de curto e longo prazo. √â um tipo de rede neural recorrente (RNN) que consegue **memorizar informa√ß√µes por longos per√≠odos de tempo**.

    √â interessante destacar o pr√©-processamento realizado para a utiliza√ß√£o do LSTM, os coment√°rios no c√≥digo j√° explicam para que cada funcionalidade serve:

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec7/pre_processing_lstm.png)

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec7/pre_processing_lstm2.png)

    Outra interessante alternativa, √© realizar a an√°lise de sentimentos com o **Vader**.  Vader trabalha com uma an√°lise de sentimentos **baseado em regras**, portanto √© interessante saber alguns conceitos:

    - `Compound`: m√©trica que representa o **sentimento geral** de um texto. √â um valor real entre -1 (muitonegativo) e 1 (muito positivo).
    - `Polarity`: Mede o grau de **positividade ou negatividade** de um texto. Valor real entre -1 (muito negativo) e 1 (muito positivo).
    - `Subjectivity`: O quanto a senten√ßa se refere a **opini√£o**, **julgamento** ou **emo√ß√£o**. Valor real entre 0 e 1, quando mais pr√≥ximo de 1, mais subjetivo.

    Exemplos:

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec7/vader.png) 

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec7/vader2.png) 

- **Se√ß√£o 8: Transformers, GPT (do ChatGPT) e Bert**<br>
Na se√ß√£o 8, trabalhei com os `Transformers`, que s√£o um tipo de arquitetura de rede neural baseada em **aten√ß√£o** que revolucionou o processamento de linguagem natural (NLP). Sua principal funcionalidade √© **processar** e **entender** **sequ√™ncias de texto** de maneira eficiente, usando o mecanismo de aten√ß√£o para capturar **depend√™ncias** entre palavras. Isso permite que ele traduza, gere, resuma e classifique textos com alta precis√£o.


    As principais caracter√≠sticas s√£o:
    - `Self-Attention`: tem a fun√ß√£o de descobrir a **rela√ß√£o entre as palavras**. O transformer calcula a representa√ß√£o e a rela√ß√£o de cada palavra.
    - `Multi-Attention`: Em vez de um processo Attention, m√∫ltiplos processos s√£o processados e o resultado √© somado.
    - `Processamento Paralelo`: Diferente de RNNs/LSTMs, os Transformers** n√£o precisam processar palavras em sequ√™ncia**, tornando-os muito mais r√°pidos.
    - `Transfer Learning`: Modelos como BERT e GPT podem ser **pr√©-treinados em grandes quantidades de dados** e depois **ajustados para tarefas espec√≠ficas**.

    A **arquitetura** do Transformer √© dividida em duas partes principais:
    - `Encoder (Codificador)`: Extrai **significado** do texto de entrada.
    - `Decoder (Decodificador)`: Gera a **sa√≠da baseada no contexto aprendido** pelo encoder.

    Outro conceito muito importante que √© basedo em **transformers** √© o `BERT`, um modelo de linguagem capaz de entender o **contexto completo de uma palavra dentro de uma frase**, considerando tanto as palavras da esquerda, quanto da direita.

    O BERT possui v√°rias variantes com diversas caract√©ricas como serem mais leves, ou mais r√°pidas ou menores, entre outras.

    Nesta se√ß√£o utilizei da plataforma colaborativa `HuggingFace` que possui uma gama muito vasta de modelos capazes de lidar com processamento de texto, a√∫dio e imagem. A plataforma possui uma lib chamada `Transformers`, que possui v√°rias funcionalidades √∫teis, como por exemplo, o `Pipeline`. 
    
    Um **pipeline** √© uma abstra√ß√£o que **encapsula todo o fluxo de entrada e sa√≠da para uma tarefa espec√≠fica de PLN**. Ele cuida de toda a pr√©-processamento, infer√™ncia e p√≥s-processamento, permitindo que voc√™ use modelos sem precisar lidar diretamente com as complexidades da arquitetura.

    Exemplo de um pipeline **Q&A(Question Answering)**:

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec8/pipeline_q&a.png)

    Outro pipeline interessante √© o de **preenchimento de lacunas(Fill Mask)**. Ele d√° algumas sugest√µes de palavras para completar a lacuna com um score para cada uma:

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec8/pipeline_fillmask.png)

    Essas sugest√µes de onde jogar o lixo n√£o foram muito boas... üòÖ

    Outra plataforma trabalhada nessa se√ß√£o foi a **OpenAI** para utiliza√ß√£o do modelo `GPT-3`. Por√©m, como a OpenAI n√£o fornece mais free trial, n√£o √© poss√≠vel utilizar os modelos GPT.

- **Se√ß√£o 9: Modelagem de T√≥picos com Bert**<br>
Na se√ß√£o 9 aprendi sobre **modelagem de t√≥picos** utilizando o `BERT`, conceituado na se√ß√£o anterior. A modelagem de t√≥picos √© uma t√©cnica usada para descobrir automaticamente os **temas principais** dentro de um grande conjunto de textos.

    O `BERTopic` √© um modelo avan√ßado de modelagem de t√≥picos que combina **embeddings** de linguagem (como BERT) com algoritmos de clusteriza√ß√£o para identificar t√≥picos de maneira mais precisa.

    Cria√ß√£o do modelo BERTopic:

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec9/bertopic.png)

    T√≥picos encontrados:

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec9/frequency_topics.png)

    Visualiza√ß√£o dos scores de 6 palavras dos top 8 t√≥picos:

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec9/topic_words_score.png) 

- **Se√ß√£o 10: NLP com Spark**<br>
Na se√ß√£o 10 trablhei com o `Spark` utilizando um cluster no **Databricks Community**. Databricks √© uma plataforma otimizada para utiliza√ß√£o de Spark, onde NLP pode ser utilizado usando Deep Learning e embeddings de BERT, RoBERTa, Word2Vec, etc.

    Para estudar um pouco sobre como funcionava NLP dentro do Spark, o professor utilizou a mesma base spam.csv de outras se√ß√µes.

    Transforma√ß√£o da vari√°vel **Category**(semelhante ao labelencoder):

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec10/category_transform.png)

    Tokeniza√ß√£o da vari√°vel **Message**:

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec10/message_transform.png)

    Utilizando o `Word2Vec` para representar a vari√°vel **Message** vetorialmente.

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec10/w2v_vetorial.png)

    Ap√≥s o treinamento do RandomForestClassifier, as previs√µes foram geradas. √â interessante observar que o Spark cria 3 colunas adicionais: **rawPrediction**, **probability** e **prediction**.

    - `rawPrediction`: Soma das probabilidades dos votos de **todas as √°rvores do Random Forest**. Representa a contagem ponderada dos votos para cada classe.
    - `probability`:  **Normaliza√ß√£o** da rawPrediction, convertendo-a em uma distribui√ß√£o de probabilidades entre 0 e 1 para cada classe.
    - `prediction`: Classe **final prevista**, baseada na classe com maior probabilidade em probability.

    ![Evidencia](./evidencias/curso_formacao_processamento_nlp/sec10/previsions.png)


### üß† Curso: Face Recognition with Machine Learning + Deploy Flask App

### üß† Curso: MLOps: Implanta√ß√£o e Opera√ß√£o de Modelos de Machine Learning



# üë®üèº‚Äçüéì Certificados

### üß† Curso: Forma√ß√£o Processamento de Linguagem Natural, LLMs e Gen AI

### üß† Curso: Face Recognition with Machine Learning + Deploy Flask App

### üß† Curso: MLOps: Implanta√ß√£o e Opera√ß√£o de Modelos de Machine Learning